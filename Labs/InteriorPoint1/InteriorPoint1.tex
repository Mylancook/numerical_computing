\lab{Interior Point I}{Interior Point I}

% Bold mu and lambda symbols for this lab.
\def\Mu{\boldsymbol{\mu}}
\def\Lamb{\boldsymbol{\lambda}}

\objective{For decades after its invention, the Simplex algorithm was the only competitive method for linear programming. The past 30 years, however, have seen the discovery and widespread adoption of a new family of algorithms that rival--and in some cases outperform--the Simplex algorithm, collectively called \emph{Interior Point methods}. One of the major shortcomings of the Simplex algorithm is that the number of steps required to solve the problem can grow exponentially with the size of the linear system. Thus, for certain large linear programs, the Simplex algorithm is simply not viable. Interior Point methods offer an alternative approach and enjoy much better theoretical convergence properties. In this lab we implement an Interior Point method for linear programs, and in the next lab we will turn to the problem of solving quadratic programs.}

\section*{Introduction}

Recall that a linear program is a constrained optimization problem with a linear objective function and linear constraints.
The linear constraints define a set of allowable points called the \emph{feasible region}, the boundary of which forms a geometric object known as a \emph{polytope}.
The theory of convex optimization ensures that the optimal point for the objective function can be found among the vertices of the feasible polytope. The Simplex Method tests a sequence of such vertices until it finds
the optimal point.
Provided the linear program is neither unbounded nor infeasible, the algorithm is certain to produce the correct answer after a finite number of steps, but it does not guarantee an efficient path along the polytope toward the minimizer.
Interior point methods do away with the feasible polytope and instead generate a sequence of points that cut through the interior (or exterior) of the feasible region and converge iteratively to the optimal point.
Although it is computationally more expensive to compute such interior points, each step results in significant progress toward the minimizer.
See Figure \ref{fig:intPath}.
In general, the Simplex Method requires many more (less expensive) computations.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{interiorPath.pdf}
\caption{A path traced by an Interior Point algorithm.}
\label{fig:intPath}
\end{figure}

\section*{Primal-Dual Interior Point Methods}
Some of the most popular and successful types of Interior Point methods nowadays are known as Primal-Dual Interior Point methods.
Consider the following linear program:
\begin{align*}
\text{minimize }\qquad &\c^T\x\\
\text{subject to }\qquad &A\x = \b\\
&\x \geq \0.
\end{align*}
Here, $\x, \c \in \mathbb{R}^n$, $\b \in \mathbb{R}^m$, and $A$ is an $m \times n$ matrix with full row rank.
% By $x \geq 0$, we simply mean that each coordinate of $x$ is nonnegative.
% Note that this formulation is quite general, as any linear program can be posed in this manner, after appropriate transformations.
% TODO: Change \geq to the component-wise inequality that is used in the book.
This is the \emph{primal} problem, and its \emph{dual} takes the form:
\begin{align*}
\text{maximize }\qquad &\b^T\Lamb\\
\text{subject to }\qquad &A^T\Lamb + \Mu = \c\\
&\Mu,\Lamb \geq \0,
\end{align*}
where $\Lamb \in \mathbb{R}^m$ and $\Mu \in \mathbb{R}^n$.
% Changed s to \mu to match the textbook.

\subsection*{KKT Conditions}

The theory of convex optimization gives us necessary and sufficient conditions for the solutions to the primal and dual problems via the Karush-Kuhn-Tucker (KKT) conditions.
The Lagrangian for the primal problem is as follows:
\[
\CalL(\x, \Lamb, \Mu)
= \c^T\x + \Lamb^T(\b - A\x) - \Mu^T\x
\]
The KKT conditions are
\begin{align*}
A^T\Lamb + \Mu &= \c\\
A\x &= \b\\
x_i\mu_i &= 0, \quad i = 1,2,\ldots,n,\\
\x, \Mu &\geq 0.
\end{align*}
It is convenient to write these conditions in a more compact manner, by defining an almost-linear function $F$ and setting it equal to zero:
\begin{align*}
F(\x,\Lamb,\Mu) :=
\begin{bmatrix}
A^T\Lamb + \Mu - \c\\
A\x - \b\\
M\x\\
\end{bmatrix}
&= \0,\\
(\x,\Mu) &\geq 0,
\end{align*}
where $M = \text{diag}(\mu_1,\mu_2,\ldots,\mu_n)$.
Note that the first row of $F$ is the KKT condition for dual feasibility, the second row of $F$ is the KKT condition for the primal problem, and the last row of $F$ accounts for complementary slackness.

\begin{comment} % Derivation overkill vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv
First, we calculate the so-called \emph{Lagrangian function} for our linear program.
Recall that the Lagrangian function for a general constrained optimization problem of the form
\begin{align*}
\text{minimize }\qquad &f(x)\\
\text{subject to }\qquad &c_i(x) = 0, \quad i = 1,2,\ldots,k,\\
&d_i(x) \geq 0, \quad i = 1,2,\ldots,j,
\end{align*}
is defined to be
\[
\mathcal{L}(x, \lambda, s) := f(x) - \displaystyle\sum_{i=1}^k\lambda_ic_i(x) - \displaystyle\sum_{i=1}^js_id_i(x).
\]
From here, the KKT conditions are
\begin{align*}
\nabla_x\mathcal{L}(x,\lambda,s) &= 0,\\
c_i(x) &= 0,\quad i = 1,2,\ldots,k,\\
s_id_i(x) &= 0,\quad i=1,2,\ldots,j,\\
d_i(x) &\geq 0,\quad i=1,2,\ldots,j,\\
s_i &\geq 0,\quad i=1,2,\ldots,j.
\end{align*}
For our linear program, $f(x) = c^Tx$, $c_i(x) = A_i^Tx - b_i$ (where $A_i$ denotes the $i$-th row of $A$), and $d_i(x) = x_i$.
Hence, you can check that our Lagrangian function is
\[
\mathcal{L}(x,\lambda,s) = c^Tx - \lambda^T(Ax - b) - s^Tx.
\]
We then calculate
\[
\nabla_x\mathcal{L}(x,\lambda,s) = (c^T - \lambda^TA - s^T)^T = c - A^T\lambda - s.
\]
The KKT conditions can now be expressed as follows:
\begin{align*}
A^T\lambda + s &= c\\
Ax &= b\\
x_is_i &= 0, \quad i = 1,2,\ldots,n,\\
x, s &\geq 0.
\end{align*}
In fact, it is convenient to write these conditions in a more compact manner, by defining an almost-linear function $F$ and setting it equal to zero:
\begin{align*}
F(x,\lambda,s) :=
\begin{bmatrix}
A^T\lambda + s - c\\
Ax - b\\
XSe\\
\end{bmatrix}
&= 0,\\
(x,s) &\geq 0,
\end{align*}
where $X = \text{diag}(x_1,x_2,\ldots,x_n)$, $S = \text{diag}(s_1,s_2,\ldots,s_n)$, and $e = (1,1,\ldots,1)^T$.
\end{comment} % Derivation overkil ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

\begin{problem} % First step: define F(x,lamb,mu) -----------------------------
Define a function \li{interiorPoint()} that will be used to solve the complete interior point problem.
This function should accept $A$, $\b$, and $\c$ as parameters, along with the keyword argument \li{niter} that gives the number of iterations to perform.

For this problem, within the \li{interiorPoint()} function, write a function for the $F$ described above.
This function should accept $\x$, $\Lamb$, and $\Mu$ as parameters and return a 1-dimensional NumPy array with $2n+m$ entries.
\end{problem}

\subsection*{Search Direction and Step Length}

A Primal-Dual Interior Point method is a line search method that starts with an initial guess $(x_0, \lambda_0, s_0)$ and produces a sequence of points that converge to $(x^*, \lambda^*, s^*)$, the solution to the KKT equations and hence the solution to the original linear program.
%We now describe how to select the search direction and step length at each iteration of the algorithm.
The constraints on the problem make finding a search direction and step length a little more complicated than for the unconstrained line search we have studied previously.

In the spirit of Newton's Method, we can form a linear approximation of the system $F(x,\lambda,s) = 0$ centered around our current point $(x, \lambda, s)$, and calculate the direction $(\triangle x, \triangle \lambda, \triangle s)$ in which to step to set the linear approximation equal to 0.
This equates to solving the linear system:
\[
DF(x,\lambda,s)
\begin{bmatrix}
\triangle x\\
\triangle \lambda\\
\triangle s\\
\end{bmatrix}
= - F(x,\lambda,s).
\]
Here $DF(x,\lambda,s)$ denotes the total derivative matrix of $F$.
We can calculate this matrix block-wise by obtaining the partial derivatives of each block entry of $F(x,\lambda, s)$ with respect to $x$, $\lambda$, and $s$, respectively.
For example, consider the topmost block entry of $F(x,\lambda,s)$, given by
\[
A^T\lambda + s - c.
\]
We calculate the partial derivatives of this expressions as follows:
\begin{align*}
\frac{\partial}{\partial x}[A^T\lambda + s - c] &= 0,\\
\frac{\partial}{\partial \lambda}[A^T\lambda + s - c] &= A^T,\\
\frac{\partial}{\partial s}[A^T\lambda + s - c] &= I.\\
\end{align*}
Hence, the topmost block row of $DF(x,\lambda,s)$ is given by
\[
\begin{bmatrix}
0 \quad A^T \quad I\\
\end{bmatrix}.
\]

Continuing in this manner, we can calculate the entire derivative matrix, which leads us to the following linear system:
\begin{equation}
\begin{bmatrix}
0 & A^T & I\\
A & 0 & 0\\
S & 0 & X
\end{bmatrix}
\begin{bmatrix}
\triangle x\\
\triangle \lambda\\
\triangle s
\end{bmatrix}
=
\begin{bmatrix}
-r_c\\
-r_b\\
-XSe
\end{bmatrix},
\label{eq:affine}
\end{equation}
where $r_b = Ax - b$ and $r_c = A^T\lambda + s - c$.
This Newton direction is too greedy, however, and even small steps in this direction may cause us to violate the nonnegativity condition (the last line of the KKT conditions).
We need to find a new search direction.

Choosing an appropriate search direction is a tricky task, and various approaches exist.
We will follow a popular strategy known as the \emph{Predictor-Corrector Algorithm}.
The idea is to actually calculate two directions: we first calculate
the standard Newton direction described above; that is, we obtain the solution to Equation \ref{eq:affine}.
This is known as the \emph{predictor step}, since it gives us a direction in which the objective does indeed decrease, hence predicting our
final search direction.
Denote the solution to this system by $(\triangle x^a, \triangle \lambda^a, \triangle s^a)$.
As discussed, however, we must deviate from this direction somewhat, and so we additionally solve the following linear system of equations:
\begin{equation}
\begin{bmatrix}
0 & A^T & I\\
A & 0 & 0\\
S & 0 & X
\end{bmatrix}
\begin{bmatrix}
\triangle x\\
\triangle \lambda\\
\triangle s
\end{bmatrix}
=
\begin{bmatrix}
-r_c\\
-r_b\\
-XSe - \triangle X^a\triangle S^ae + \sigma \mu e
\end{bmatrix}.
\label{eq:centering}
\end{equation}
The solution to this system, which we denote by $(\triangle x, \triangle \lambda, \triangle s)$, is our final search direction.

We now describe the new pieces of Equation \ref{eq:centering}.
Note that
\begin{align*}
\triangle X^a &= \text{diag}(\triangle x_1^a,\ldots,\triangle x_n^a),\\
\triangle S^a &= \text{diag}(\triangle s_1^a,\ldots,\triangle s_n^a).
\end{align*}
Further, define $\mu := x^Ts/n$.
This quantity, called the \emph{duality measure}, tells us roughly how close we are to the optimal solution, with values closer to 0 being more desirable.
It is present in Equation \ref{eq:centering} for the purpose of obtaining a search direction that leads to a decrease towards 0 in the duality measure.

The formula for $\sigma$ is somewhat more complicated, and is based on a heuristic approach.
First, make the following calculations (which give the maximum allowable step lengths in the Newton search direction for $x$ and $s$ before the nonnegativity condition is violated):
\begin{align*}
\alpha_a^p &:= \min\left(1, \displaystyle\min_{i : \triangle x_i^a < 0}-\frac{x_i}{\triangle x_i^a}\right)\\
\alpha_a^d &:= \min\left(1, \displaystyle\min_{i : \triangle s_i^a < 0}-\frac{s_i}{\triangle s_i^a}\right).
\end{align*}
Next, define
\[
\mu_a := \frac{1}{n}(x+\alpha_a^p\triangle x^a)^T(s+\alpha_a^d\triangle s^a),
\]
which is simply the duality measure for the point obtained by taking a full step in the Newton direction.
Finally, calculate $\sigma$ by the formula
\[
\sigma = \left(\frac{\mu_a}{\mu}\right)^3.
\]

Now that we have our search direction, it remains to choose our step length.
We wish to step nearly as far as possible without violating the nonnegativity condition, thus remaining in the interior of the feasible region.
First, make the following calculations, which give the maximum allowable step lengths for $x$ and $s$, respectively:
\begin{align*}
\beta^p &:= \displaystyle\min_{i : \triangle x_i < 0}-\frac{x_i}{\triangle x_i}\\
\beta^d &:= \displaystyle\min_{i : \triangle s_i < 0}-\frac{s_i}{\triangle s_i}.
\end{align*}
Next, we back off from these maximum step lengths slightly:
\begin{align*}
\alpha^p &:= \min(1, 0.95\beta^p)\\
\alpha^d &:= \min(1, 0.95\beta^d).
\end{align*}
These are our final step lengths.
That is, our next point $(x', \lambda', s')$ is given by
\begin{align*}
x' &= x + \alpha^p\triangle x\\
(\lambda', s') &= (\lambda, s) + \alpha^d(\triangle \lambda, \triangle s).
\end{align*}
We summarize the entire procedure in Algorithm \ref{alg:predcorr}.
\begin{algorithm}
\begin{algorithmic}[1]
\Procedure{Predictor-Corrector Algorithm}{}
    \State \textrm{Choose initial point } $(x_0, \lambda_0, s_0)$.
    \For{$k = 0, 1, 2, \ldots$}
        \State \textrm{Solve for } $(\triangle x^a, \triangle \lambda^a, \triangle s^a)$.
        \State \textrm{Calculate } $\alpha_a^p, \alpha_a^d, \mu_a$, \textrm{and} $\sigma$.
        \State \textrm{Solve for } $(\triangle x, \triangle \lambda, \triangle s)$.
        \State \textrm{Calculate the step lengths } $\alpha^p, \alpha^d$.
        \State $x_{k+1} = x_k + \alpha^p\triangle x$,\\
        $\qquad\quad(\lambda_{k+1}, s_{k+1}) = (\lambda_k, s_k) + \alpha^d(\triangle \lambda, \triangle s)$.
    \EndFor
\EndProcedure
\end{algorithmic}
\caption{Predictor-Corrector Algorithm}
\label{alg:predcorr}
\end{algorithm}

A few notes on the implementation of this algorithm are in order.
In each iteration, by far the most expensive operations are solving Equations \ref{eq:affine} and \ref{eq:centering}.
Fortunately, the only difference between the two equations is the right-hand side.
Thus, we can avoid repetitious calculation by first factorizing the block matrix
\[
\begin{bmatrix}
0 & A^T & I\\
A & 0 & 0\\
S & 0 & X
\end{bmatrix}
\]
once at the beginning of the iteration, and then using the factorization twice to solve both equations.
For convenience, consider using the functions \li{lu_factor()} and \li{lu_solve()} from the \li{scipy.linalg} module.
It is possible to speed up these calculations further, but for simplicity we won't pursue the issue further.

Next, as mentioned above, the duality measure $\mu$ tells us in some sense how close our current point is to the minimizer.
The closer $\mu$ is to 0, the closer we are to the optimal point.
Thus, by printing the value of $\mu$ at each iteration, you can track how your algorithm is progressing and detect when you have converged.

Another potentially tricky calculation that comes up in each iteration has the following form:
\[
\min\left(1, \displaystyle\min_{i : u_i < 0}-\frac{v_i}{u_i}\right),
\]
where $\mathbf{u} = (u_1, \ldots, u_n)^T$ and $\mathbf{v} = (v_1, \ldots, v_n)^T$ are vectors.
This can be done in a vectorized fashion in Python as follows (assuming that the arrays \li{u} and \li{v} have already been initialized):
\begin{lstlisting}
>>> mask = u < 0
>>> if np.any(mask):
>>>     myMin = min(1, (-v/u)[mask].min())
>>> else:
>>>     myMin = 1
\end{lstlisting}
We need the \li{if} statement to deal with the case where no entry of $u$ is negative.

Finally, the choice of initial point $(x_0, \lambda_0, s_0)$ is an important, nontrivial one.
A naively or randomly chosen initial point may cause the algorithm to fail to converge.
We provide code to calculate an appropriate initial point.
You should include a call to this function in your algorithm to obtain the initial point.

\begin{lstlisting}
def startingPoint(A, b, c):
    """
    Calculate an initial guess to the solution of the
    linear program min c^T x, Ax = b, x>=0.
    Inputs:
        A -- array of shape (m,n) with linearly independent rows
        b -- array of length m
        c -- array of length n
    Returns:
        x -- array of length n
        lam -- array of length m
        s -- array of length n
    Ref: Nocedal and Wright, p. 410
    """
    # first calculate x, lam, s of minimal norm satisfying the primal and dual constraints
    B = la.inv(A.dot(A.T))
    x = A.T.dot(B.dot(b))
    lam = B.dot(A.dot(c))
    s = c - A.T.dot(lam)

    # perturb x and s so they are nonnegative
    dx = max((-3./2)*x.min(), 0)
    ds = max((-3./2)*s.min(), 0)
    x += dx*np.ones(x.shape)
    s += ds*np.ones(s.shape)

    # perturb x and s so they are not too close to zero, not too dissimilar
    dx = .5*(x*s).sum()/s.sum()
    ds = .5*(x*s).sum()/x.sum()
    x += dx*np.ones(x.shape)
    s += ds*np.ones(s.shape)

    return x, lam, s
\end{lstlisting}

\begin{problem} % Split up this problem into several parts!
Write a function \li{interiorPoint} that implements the interior point method described above.
The function should accept $A$, $b$, and $c$ as parameters, along with keyword arguments
\li{niter} that gives the number of iterations to perform, and \li{verbose} that indicates
whether to print out the value of the objective function and $\mu$ at each iteration.
The function should return the optimal point $x$ along with the value of the objective function at this
point.
\end{problem}

\section*{Least Absolute Deviations}
We now return to the familiar problem of fitting a line (or hyperplane) to a set of data.
We have previously approached this problem by minimizing the sum of the squares of the errors between the data points and the line, an approach known as \emph{least squares}.
The least squares solution can be obtained analytically when fitting a linear function, or through a number of optimization methods (such as Conjugate Gradient) when fitting a nonlinear function.

The method of least absolute deviations also seeks to find a best fit line to a set of data, but the error between the data and the line is measured differently.
In particular, suppose we have a set of data points $(y_1, \mathbf{x}_1), (y_2, \mathbf{x}_2), \ldots, (y_m, \mathbf{x}_m)$, where $y_i \in \mathbb{R}$, $\mathbf{x}_i \in \mathbb{R}^n$ for $i = 1, 2, \ldots, m$.
Here, the $\mathbf{x}_i$ vectors are the explanatory variables and the $y_i$ values are the response variables, and we assume the following linear model:
\[
y_i = \beta^T\mathbf{x}_i + b, \qquad i = 1, 2, \ldots, m,
\]
where $\beta\in\mathbb{R}^n$ and $b \in \mathbb{R}$.
The error between the data and the proposed linear model is given by
\[
\sum_{i=1}^n |\beta^T\mathbf{x}_i + b - y_i|,
\]
and we seek to choose the parameters $\beta, b$ so as to minimize this error.

Before we explore how to solve this problem, a discussion of the differences between least absolute deviations and least squares is in order.
The most prominent difference between these two approaches is how they respond to outliers in the data.
Least absolute deviations is robust in the presence of outliers, meaning that one (or a few) errant data points won't severely affect the fitted line.
Indeed, in most cases, the best fit line is guaranteed to pass through at least two of the data points.
This is a desirable property when the outliers may be ignored (perhaps because they are due to measurement error or corrupted data).
Least squares, on the other hand, is much more sensitive to outliers, and so is the better choice when outliers cannot be dismissed.
See Figure \ref{fig:leastAbsDev}.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{leastAbsDev.pdf}
\caption{Fitted lines produced by least absolute deviations (top) and least squares (bottom). The presence of an outlier accounts for the
stark difference between the two lines.}
\label{fig:leastAbsDev}
\end{figure}

While least absolute deviations is robust with respect to outliers, small horizontal perturbations of the data points can lead to very different fitted lines.
Hence, the least absolute deviations solution is less stable than the least squares solution.
In some cases there are even infinitely many lines that minimize the least absolute deviations error term.
However, one can expect a unique solution in most cases.

The least absolute deviations solution arises naturally when we assume that the residual terms $\beta^T\mathbf{x}_i + b - y_i$ have a particular statistical distribution (the Laplace distribution).
Ultimately, however, the choice between least absolute deviations and least squares depends on the nature of the data at hand, as well as your own good judgment.

\subsection{Least Absolute Deviations as a Linear Program}
We can formulate the least absolute deviations problem as a linear program, and then solve it using our interior point method.
For $i = 1, 2, \ldots, m$ we introduce the artificial variable $u_i$ to take the place of the error term $|\beta^T\mathbf{x}_i + b - y_i|$, and we require this variable to satisfy $u_i \geq |\beta^T\mathbf{x}_i + b - y_i|$.
This constraint is not yet linear, but we can split it into an equivalent set of two linear constraints:
\begin{align*}
u_i &\geq \beta^T\mathbf{x}_i + b - y_i,\\
u_i &\geq y_i - \beta^T\mathbf{x}_i - b.
\end{align*}
The $u_i$ are implicitly constrained to be nonnegative.

Our linear program can now be stated as follows:
\begin{align*}
\text{minimize }\qquad &\sum_{i=1}^m u_i\\
\text{subject to }\qquad &u_i \geq \beta^T\mathbf{x}_i + b - y_i,\\
&u_i \geq y_i - \beta^T\mathbf{x}_i - b.
\end{align*}
This is not yet in the form that we need in order to use our interior point method.
For each inequality constraint, we bring all variables ($u_i, \beta, b$) to the left hand side and introduce a nonnegative slack variable to transform the constraint into an equality:
\begin{align*}
u_i  - \beta^T\mathbf{x}_i - b - s_{2i-1}&= -y_i,\\
u_i +\beta^T\mathbf{x}_i + b - s_{2i}&= y_i,\\
s_{2i-1}, s_{2i}&\geq 0.
\end{align*}

Notice that the variables $\beta, b$ are not assumed to be nonnegative, but in our interior point method, all variables are assumed to be nonnegative.
We can fix this situation by writing these variables as the difference of nonnegative variables:
\begin{align*}
  \beta &= \beta_1 - \beta_2,\\
  b &= b_1 - b_2,\\
  &\beta_1, \beta_2, b_1, b_2 \geq 0.
\end{align*}
Substituting these values into our constraints, we have the following system of constraints:
\begin{align*}
u_i  - \beta_1^T\mathbf{x}_i + \beta_2^T\mathbf{x}_i - b_1 + b_2 - s_{2i-1}&= -y_i,\\
u_i + \beta_1^T\mathbf{x}_i - \beta_2^T\mathbf{x}_i + b_1 - b_2 - s_{2i}&= y_i,\\
u_i, \beta_1, \beta_2, b_1, b_2, s_{2i-1}, s_{2i}&\geq 0.
\end{align*}
Writing $\mathbf{y} = (-y_1, y_1, -y_2, y_2, \ldots, -y_m, y_m)^T$ and $\beta_i = (\beta_{i,1}, \ldots, \beta_{i,n})^T$ for $i = \{1, 2\}$, we can aggregate all of our variables into one vector as follows:
\[
\mathbf{v} = (u_1,\ldots, u_m, \beta_{1,1},\ldots, \beta_{1,n}, \beta_{2,1},\ldots, \beta_{2,n}, b_1, b_2, s_1,\ldots,s_{2m})^T.
\]
Defining $\mathbf{c} = (1, 1, \ldots, 1, 0, \ldots, 0)^T$ (where only the first $m$ entries are equal to 1), we can write our objective function as
\[
\sum_{i=1}^m u_i = \mathbf{c}^T\mathbf{v}.
\]
Hence, the final form of our linear program is:
\begin{align*}
  \text{minimize }\qquad &\mathbf{c}^T\mathbf{v}\\
  \text{subject to }\qquad A\mathbf{v} &= \mathbf{y},\\
  \mathbf{v} &\geq 0,
\end{align*}
where $A$ is a matrix containing the coefficients of the constraints.
Our constraints are now equalities, and the variables are all nonnegative, so we are ready to use our interior point method to obtain the solution.

Consider the following example.
We start with an array \li{data}, each row of which consists of the values $y_i, x_{i,1},\ldots,x_{i,n}$, where $\mathbf{x}_i = (x_{i,1}, x_{i,2}, \ldots, x_{i,n})^T$.
We will have $3m + 2(n+1)$ variables in our linear program.
Below, we initialize the vectors $\mathbf{c}$ and $\mathbf{y}$.
\begin{lstlisting}
>>> m = data.shape[0]
>>> n = data.shape[1] - 1
>>> c = np.zeros(3*m + 2*(n + 1))
>>> c[:m] = 1
>>> y = np.empty(2*m)
>>> y[::2] = -data[:, 0]
>>> y[1::2] = data[:, 0]
>>> x = data[:, 1:]
\end{lstlisting}

The hardest part is initializing the constraint matrix correctly.
It has $2m$ rows and $3m + 2(n+1)$ columns.
Try writing out the constraint matrix by hand for small $m, n$, and make sure you understand why the code below is correct.
% TODO: Do this with NumPy stacking.
\begin{lstlisting}
>>> A = np.ones((2*m, 3*m + 2*(n + 1)))
>>> A[::2, :m] = np.eye(m)
>>> A[1::2, :m] = np.eye(m)
>>> A[::2, m:m+n] = -x
>>> A[1::2, m:m+n] = x
>>> A[::2, m+n:m+2*n] = x
>>> A[1::2, m+n:m+2*n] = -x
>>> A[::2, m+2*n] = -1
>>> A[1::2, m+2*n+1] = -1
>>> A[:, m+2*n+2:] = -np.eye(2*m, 2*m)
\end{lstlisting}

Now we can calculate the solution by calling our interior point function.
\begin{lstlisting}
>>> sol = interiorPoint(A, y, c, niter=10, verbose=False)[0]
\end{lstlisting}

The variable \li{sol}, however, holds the value for the vector
\[
\mathbf{v} = (u_1,\ldots, u_m, \beta_{1,1},\ldots, \beta_{1,n}, \beta_{2,1},\ldots, \beta_{2,n}, b_1, b_2, s_1,\ldots,s_{2m+1})^T.
\]
We want the values of $\beta = \beta_1-\beta_2$ and $b = b_1 - b_2$, and we accomplish this with the following:
\begin{lstlisting}
>>> beta = sol[m:m+n] - sol[m+n:m+2*n]
>>> b = sol[m+2*n] - sol[m+2*n+1]
\end{lstlisting}

\begin{problem} % Least Total Deviations Problem
The file \li{simdata.txt} contains two columns of data.
The first gives the values of the response variables, and the second column gives the values of the explanatory variables.
Find the least absolute deviations line for this data set, and plot it together with the data.
\end{problem}
