\lab{Linear Transformations}{Linear Transformations}
\objective{One of the most important questions in scientific computing is ``How long will a computer take to execute this algorithm?'' In this lab we explore why NumPy is a good option for performing numerical linear algebra.
Linear transformations are the most basic and essential operations in vector space theory; we visually explore how linear transformations alter points in $\mathbb{R}^2$.
}
% \objective{Apply affine transformations to a set of vectors in $\mathbb{R}^2$.}
% \objective{Introduce the temporal and spatial complexity and explore SciPy's methods for working with sparse matrices.}

\section*{Matrix Operations} % ================================================

\subsection*{Timing Code} % ---------------------------------------------------

The \li{time} module in the standard library include functions for dealing with time.
The module's \li{time()} function measures the number of seconds from a fixed starting point, called ``the Epoch'' (January 1, 1970 for Unix machines).

\begin{lstlisting}
>>> import time
>>> time.time()
1466609325.819298
\end{lstlisting}

The \li{time()} function\footnote{The \li{clock()} function is similar to \li{time()}, but it records more precision on Windows machines.} is useful for measuring how long it takes for code to run: record the time just before and just after the code in question, then subtract the first measurement from the second to get the number of seconds that have passed.

\begin{lstlisting}
>>> def time_for_loop(iters):
...     """Time how long it takes to go through 'iters' iterations of nothing."""
...     start = time.time()         # Clock the starting time.
...     for _ in range(int(iters)):
...         pass
...     end = time.time()           # Clock the ending time.
...     return end - start          # Report the difference.
...
>>> time_for_loop(1e5)              # 1e5 = 100000.
0.007936954498291016
>>> time_for_loop(1e7)              # 1e7 = 10000000.
0.8008430004119873
\end{lstlisting}

The standard library's \li{timeit} module is built specifically to time code and has more sophisticated tools than the \li{time} module.
The \li{timeit()} function accepts a function handle (the name of the function to run) and the number of times to run it.
Additionally, in IPython the quick command \li{\%timeit} uses \li{timeit.timeit()} to quickly time a single line of code.

\begin{lstlisting}
In [1]: import timeit
In [2]: def for_loop():
   ...:     """Go through 1e7 iterations of nothing."""
   ...:     for _ in range(int(1e7)):
   ...:         pass

In [3]: timeit.timeit(for_loop, number=5) / 5.
Out[3]: 0.8081045627593995

In [4]: %timeit for_loop()
1 loop, best of 3: 801 ms per loop
\end{lstlisting}

\subsection*{Timing an Algorithm} % -------------------------------------------

Most algorithms have at least one input that dictates the size of the problem to be solved.
For example, the following functions take in a single integer $n$ and produce a random vector of length $n$ as a list or a random $n\times n$ matrix as a list of lists:

\begin{lstlisting}
from random import random

def random_vector(n):
    """Generate a random vector of length n as a list."""
    return [random() for i in xrange(n)]

def random_matrix(n):
    """Generate a random nxn matrix as a list of lists."""
    return [[random() for j in xrange(n)] for i in xrange(n)]
\end{lstlisting}

Executing \li{random_vector(n)} calls \li{random()} $n$ times, so doubling $n$ should about double the amount of time \li{random_vector(n)} takes to execute.
By contrast, executing \li{random_matrix(n)} calls \li{random()} $n^2$ times ($n$ times per row with $n$ rows).
Therefore doubling $n$ will likely more than double the amount of time \li{random_matrix(n)} takes to execute, especially if $n$ is large.

To visualize this phenomenon, we time \li{random_matrix()} for $n = 2^1,\ 2^2,\ \ldots,\ 2^{12}$ and plot $n$ against the execution time.
The result is displayed below on the left.% in Figure \ref{fig:matrix_time_result1}.

\begin{lstlisting}
>>> from matplotlib import pyplot as plt
>>> domain = 2**np.arange(1,13)
>>> times = []
>>> for n in domain:
...     start = time.time()
...     random_matrix(n)
...     times.append(time.time() - start)
...
>>> plt.plot(domain, times, 'g.-', linewidth=2, markersize=15)
>>> plt.xlabel("n", fontsize=14)
>>> plt.ylabel("Seconds", fontsize=14)
>>> plt.show()
\end{lstlisting}

\begin{figure}[H] % Generated with timing_demo() in plots.py.
\captionsetup[subfigure]{justification=centering}
\centering
\begin{subfigure}{.5\textwidth}
    \centering
    \includegraphics[width=\linewidth]{time_random_matrix1.pdf}
    % \label{fig:matrix_time_result1}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
    \centering
    \includegraphics[width=\linewidth]{time_random_matrix2.pdf}
\end{subfigure}
\end{figure}

The figure on the left shows that the execution time for \li{random_matrix(n)} increases quadratically in $n$.
In fact, the blue dotted line in the figure on the right is the parabola $y = an^2$, which fits nicely over the timed observations. Here $a$ is a small constant, but it is much less significant than the exponent on the $n$---we can leave it off altogether and write \li{random_matrix(n)} $\sim n^2$.%\footnote{In precise mathematical terms, we might write \li{random_matrix(n)} $\in O(n^2)$. See Volume II for more details on Big-O notation.}

% TODO: More explanation here? Reference to Vol II?

\begin{problem} % Time Matrix-Vector and Matrix-Matrix Multiplication.
Let $A$ be an $m \times n$ matrix with entries $a_{ij}$, $\x$ be an $n \times 1$ vector with entries $x_k$, and $B$ be an $n \times p$ matrix with entries $b_{ij}$.
%
\begin{align*}
A = \left[\begin{array}{cccc}
a_{11} & a_{12} & \cdots & a_{1n} \\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{m1} & a_{m2} & \cdots & a_{mn}
\end{array}\right]
&&
\x = \left[\begin{array}{c}
x_1 \\ x_2 \\ \vdots \\ x_n
\end{array}\right]
&&
B = \left[\begin{array}{cccc}
b_{11} & b_{12} & \cdots & b_{1p} \\
b_{21} & b_{22} & \cdots & b_{2p} \\
\vdots & \vdots & \ddots & \vdots \\
b_{n1} & b_{n2} & \cdots & b_{np}
\end{array}\right]
\end{align*}

The matrix-vector product $A\x = \y$ is a new $m \times 1$ vector and the matrix-matrix product $AB = C$ is a new $m \times p$ matrix.
The entries $y_i$ of $\y$ and $c_{ij}$ of $C$ are determined by the following formulas:
%
\begin{align*}
y_i = \sum_{k=1}^n a_{ik}x_k%,\qquad i = 1,\ 2,\ \ldots,\ m.
&&
c_{ij} = \sum_{k=1}^n a_{ik}b_{kj}%,\quad i = 1,\, 2,\, \ldots,\ m, \quad j = 1,\, 2\, \ldots,\ l.
\end{align*}

Below, we implement these multiplication formulas without using NumPy.

\begin{lstlisting}
def matrix_vector_product(A, x):
    """Compute the matrix-vector product Ax (as a list)."""
    m, n = len(A), len(x)
    return [sum([A[i][k] * x[k] for k in range(n)]) for i in range(m)]

def matrix_matrix_product(A, B):
    """Compute the matrix-matrix product AB (as a list of lists)."""
    m, n, p = len(A), len(B), len(B[0])
    return [[sum([A[i][k] * B[k][j] for k in range(n)])
                                    for j in range(p) ]
                                    for i in range(m) ]
\end{lstlisting}

Use \li{time.time()}, \li{timeit.timeit()}, or \li{\%timeit} to time each of these functions with increasingly large inputs.
Generate the inputs $A$, $\x$, and $B$ with \li{random_matrix()} and \li{random_vector()} (so each input will be $n \times n$ or $n \times 1$).
Only time the multiplication functions, not the generating functions.

Report your findings in a single figure with two subplots: one with matrix-vector times, and one with matrix-matrix times.
Choose a domain for $n$ so that your figure accurately describes the growth, but avoid values of $n$ that lead to execution times of more than 1 minute.
Your figure should resemble the following plots.

\begin{figure}[H] % Generated with prob1_solution() in plots.py.
\captionsetup[subfigure]{justification=centering}
\centering
\begin{subfigure}{.5\textwidth}
    \centering
    \includegraphics[width=\linewidth]{matrixVectorMultiplication.pdf}
\end{subfigure}%
\begin{subfigure}{.474\textwidth}
    \centering
    \includegraphics[width=\linewidth]{matrixMatrixMultiplication.pdf}
\end{subfigure}
\end{figure}

\label{prob:matrix-multiplication-timing}
\end{problem}

\subsection*{Logarithmic Plots} % ---------------------------------------------

The two plots from Problem \ref{prob:matrix-multiplication-timing} look similar, but the actual execution times differ greatly.
To adequately compare the two, we need to view the results differently.

A \emph{logarithmic plot} uses a logarithmic scale---with values that increase exponentially, such as $10^1,\ 10^2,\ 10^3,\ \ldots$---on one or both of its axes.
The three kinds of log plots are listed below.

\begin{itemize}
\item \textbf{log-lin}: the $x$-axis uses a logarithmic scale but the $y$-axis uses a linear scale.\\
Use \li{plt.semilogx()} instead of \li{plt.plot()}.
\item \textbf{lin-log}: the $x$-axis is uses a linear scale but the $y$-axis uses a log scale.\\
Use \li{plt.semilogy()} instead of \li{plt.plot()}.
\item \textbf{log-log}: both the $x$ and $y$-axis use a logarithmic scale.\\
Use \li{plt.loglog()} instead of \li{plt.plot()}.
\end{itemize}

Since the domain $n = 2^1,\ 2^2,\ \ldots$ is a logarithmic scale and the execution times increase quadratically, we visualize the results of the previous problem with a log-log plot.
The default base for the logarithmic scales on logarithmic plots in Matplotlib is $10$.
To change the base to $2$ on each axis, specify the keyword arguments \li{basex=2} and \li{basey=2}.

Suppose the domain of $n$ values are stored in \li{domain} and the corresponding execution times for \li{matrix_vector_product()} and \li{matrix_matrix_product()} are stored in \li{vector_times} and \li{matrix_times}, respectively.
The following code produces Figure \ref{fig:loglogdemo}.

\begin{lstlisting}
>>> plt.subplot(121)        # Plot both curves on a lin-lin plot.
>>> plt.plot(domain, vector_times, 'b.-', lw=2, ms=15, label="Matrix-Vector")
>>> plt.plot(domain, matrix_times, 'g.-', lw=2, ms=15, label="Matrix-Matrix")
>>> plt.legend(loc="upper left")

>>> plot.subplot(122)       # Plot both curves on a base 2 log-log plot.
>>> plt.loglog(domain, vector_times, 'b.-', basex=2, basey=2, lw=2, ms=15)
>>> plt.loglog(domain, matrix_times, 'g.-', basex=2, basey=2, lw=2, ms=15)
>>> plt.show()
\end{lstlisting}

\begin{figure}[H] % Generated with loglog_demo() in plots.py.
\captionsetup[subfigure]{justification=centering}
\centering
\begin{subfigure}{.5\textwidth}
    \centering
    \includegraphics[width=\linewidth]{loglogDemoBad.pdf}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
    \centering
    \includegraphics[width=\linewidth]{loglogDemoGood.pdf}
\end{subfigure}
\caption{ }
\label{fig:loglogdemo}
\end{figure}

In the log-log plot, the slope of the \li{matrix_matrix_product()} line is noticeably greater than the slope of the \li{matrix_vector_product()} line.
This reflects the fact that matrix-matrix multiplication (which uses 3 \li{for} loops) is $\sim n^3$, while matrix-vector multiplication (which only has 2 loops) is only $\sim n^2$.

\begin{problem} % Why NumPy ROCKS.
NumPy is built specifically for fast numerical computations.
Repeat the experiment of Problem \ref{prob:matrix-multiplication-timing}, timing the following operations:
%
\begin{itemize}
\item matrix-vector multiplication with \li{matrix_vector_product()}.
\item matrix-matrix multiplication with \li{matrix_matrix_product()}.
\item matrix-vector multiplication with \li{np.dot()}.
\item matrix-matrix multiplication with \li{np.dot()}.
\end{itemize}

Create a single figure with two subplots: one with all four sets of execution times on a regular linear scale, and one with all four sets of execution times on a log-log scale.
Compare your results to Figure \ref{fig:loglogdemo}.
\label{prob:numpy-is-awesome}
\end{problem}

The results of Problem \ref{prob:numpy-is-awesome} show that matrix multiplication operations are significantly faster in NumPy than in plain Python.
Matrix-matrix multiplication grows cubically regardless of the implementation, but with lists of lists the times grows at a rate of $an^3$ while with NumPy the times grow at a rate of $bn^3$ where $a$ is much bigger than $b$.
% Since the exponent on the $n$ is the same in both cases, the disparity between the coefficients makes a significant impact.

\begin{info} % Note about Caching.
% Iterating through loops is very expensive.
% NumPy also uses loops, but it uses C loops instead of Python loops.
The execution times for matrix multiplication with NumPy seem to increase somewhat inconsistently.
% The fastest levels of computer memory can only handle so much information before it has to turn over to the next level of computer memory that is harder and slower to access.
NumPy operations are optimized for computer hardware.
Below, we plot execution times for vector-vector multiplication with NumPy.
The spikes in the plot indicate the times that the array no longer fits in the current level of memory
%
\begin{figure}[H]
\includegraphics[width=.5\textwidth]{cachingDemo.pdf}
\end{figure}
\end{info}

\section*{Linear Transformations} % ===========================================

% TODO: does Volume 1 use L or T for linear transformations?

A \emph{linear transformation} is a mapping between vector spaces that preserves addition and scalar multiplication.
More precisely, for arbitrary vector spaces $V$ and $W$, the mapping $L:V\rightarrow W$ is a linear transformation if and only if it satisfies the following conditions:
%
\begin{itemize}
\item $L(\x + \y) = L\x + L\y$ for any two vectors $\x,\ \y \in V$
\item $L(\alpha\x) = \alpha L\x$ for any vector $\x \in V$ and any scalar $\alpha \in \mathbb{F}$.
\end{itemize}

% TODO: Does V or W have to be finite-dimensional, or both? Look in 540 book.
Every linear transformation $L$ from an $m$-dimensional vector space to an $n$-dimensional vector space can be represented by an $m\times n$ matrix $A$.
To apply $L$ to a vector $\x$, left multiply by the matrix representation.
% So if $L\x \mapsto \y$, then we calculate $A\x = \y$.

Linear transformations can be interpreted geometrically.

The array stored in \texttt{horse.txt} has two rows: one for $x$-coordinates, and one for $y$-coordinates.

\begin{align*}
\left[\begin{array}{cccc}
x_1 & x_2 & \ldots & x_m \\
y_1 & y_2 & \ldots & y_m \\
\end{array}\right]
\end{align*}

\begin{lstlisting}
# Load the array from the .npy file.
>>> import numpy as np
>>> data = np.load("horse.npy")

# Plot the horse with black pixels.
>>> plt.plot(data[0], data[1], 'k,')

# Set the window limits to [-1, 1] by [-1, 1] and make the window square.
>>> plt.axis([-1,1,-1,1])
>>> plt.gca().set_aspect("equal")
\end{lstlisting}

There are several types of linear transformations.
We define each and give examples in $\mathbb{R}^2$.

\begin{itemize}

\item \textbf{Stretch}: %$L(x,y) \mapsto (ax,by)$.
Stretches or compresses the vector along each axis.
The matrix representation is diagonal:
%
\begin{align*}
\left[\begin{array}{rr}
a & 0  \\
0 & b
\end{array}\right].
\end{align*}
%
If $a=b$, the transformation is called a \emph{dilation}.
The stretch in Figure \ref{fig:linearly-transformed-horses} uses $a = 1/2$ and $b = 6/5$ to compress the $x$-axis and stretch the $y$-axis.

\item \textbf{Shear}: %$L(x,y) \mapsto (x + ay, y)$ or $L(x,y) \mapsto (x, bx + y)$.
Slants the vector by a scalar factor horizontally or vertically.
There are two matrix representations:
% The corresponding matrix is a Type III elementary matrix.
%
\begin{align*}
\text{horizontal shear:\ }
\left[\begin{array}{cc}
1 & a\\
0 & 1
\end{array}\right]
&&
\text{vertical shear:\ }
\left[\begin{array}{cc}
1 & 0\\
b & 1
\end{array}\right].
\end{align*}
%
Horizontal shears skew the $x$-coordinate of the vector while vertical shears skew the $y$-coordinate.
Figure \ref{fig:linearly-transformed-horses} has a horizontal shear with $a=1/2$.


\item \textbf{Reflection}: Reflects the vector about a line that passes through the origin.
% Also sometimes called a \emph{Householder transformation}.
The reflection about the line spanned by the vector $\left[a, b\right]^T$ has the matrix representation
%
\begin{align*}
\frac{1}{a^2 + b^2}
\left[\begin{array}{cc}
a^2 - b^2 & 2ab \\
2ab       & b^2 - a^2
\end{array}\right].
\end{align*}
%
The reflection in Figure \ref{fig:linearly-transformed-horses} reflects the image about the $y$-axis ($a=0$, $b=1$).

\item \textbf{Rotation}: %$L(x,y) \mapsto (x\cos\theta-y\sin\theta,\ x\sin\theta + y\cos\theta)$.
Rotates the vector around the origin.
A counterclockwise rotation of $\theta$ radians has the following matrix representation:
%
\begin{align*}
\left[\begin{array}{rr}
\cos\theta & -\sin\theta\\
\sin\theta &  \cos\theta
\end{array}\right].
\end{align*}
%
A negative value of $\theta$ performs a clockwise rotation.
Choosing $\theta = \pi/2$ produces the rotation in Figure \ref{fig:linearly-transformed-horses}.

\end{itemize}

\begin{figure}[h] % Generated with horse_drawings() in plots.py.
\captionsetup[subfigure]{justification=centering}
\centering
\begin{subfigure}{.32\textwidth}
    \centering
    \includegraphics[width=\linewidth]{OriginalHorse.pdf}
\end{subfigure}
%
\begin{subfigure}{.32\textwidth}
    \centering
    \includegraphics[width=\linewidth]{StretchHorse.pdf}
\end{subfigure}
%
\begin{subfigure}{.32\textwidth}
    \centering
    \includegraphics[width=\linewidth]{ShearHorse.pdf}
\end{subfigure}
\\
\begin{subfigure}{.32\textwidth}
    \centering
    \includegraphics[width=\linewidth]{ReflectionHorse.pdf}
\end{subfigure}
%
\begin{subfigure}{.32\textwidth}
    \centering
    \includegraphics[width=\linewidth]{RotationHorse.pdf}
\end{subfigure}
%
\begin{subfigure}{.32\textwidth}
    \centering
    \includegraphics[width=\linewidth]{CompositionHorse.pdf} % ComboHorse?
\end{subfigure}
\caption{The pixel image of \texttt{horse.npy} with various linear transformations.}
\label{fig:linearly-transformed-horses}
\end{figure}

\begin{problem} % Linear transformations.
Write a function for each of the linear transformations listed above.
Each function should accept an array to transform and the scalars that define the transformation ($a$ and $b$ for stretch, shear, and reflection, and $\theta$ for rotation).
Construct the matrix representation and left multiply to the input the array.
Return the transformed array.

To test your functions, consider writing a separate function that plots an array, transforms it, and plots the result for a side-by-side comparison.
\end{problem}

\begin{info} % Look ahead to the QR decomposition.
Reflections and rotations are two ways to implement the QR Decomposition, an important matrix factorization which we will implement in a future lab. % In the next lab.
\end{info}

\subsection*{Compositions of Linear Transformations} % ------------------------

Let $V$, $W$, and $Z$ be $m$-dimensional, $n$-dimensional, and $p$-dimensional vector spaces, respectively.
If $L:V\rightarrow W$ and $K:W\rightarrow Z$ are linear transformations with matrix representations $A$ and $B$, respectively, then the \emph{composition} $KL:V\rightarrow Z$ is also a linear transformation, and the matrix representation is the matrix product $AB$.
For example, if $S$ is a matrix representing a shear and $R$ is a matrix representing a rotation, then $RS$ represents a shear followed by a rotation.

In fact, any linear transformation $L:\mathbb{R}^2 \rightarrow\mathbb{R}^2$ is a composition of the transformations discussed above.
Figure \ref{fig:linearly-transformed-horses} displays the composition of all four previous transformations, applied in order (stretch, shear, reflection, then rotation).
% This is because reflections, dilations, and shears provide us with all the elementary matrices, and every matrix is a product of elementary matrices.

\section*{Affine Transformations} % ===========================================

A translation is a map $T: \mathbb{R}^2 \rightarrow \mathbb{R}^2$ defined by $T(\mathbf{x}) = \mathbf{x}+\mathbf{b}$ where $\mathbf{b} \in \mathbb{R}^2$.
For example, if $\mathbf{b} = (2, 0)\trp$, then applying $T$ to an image will shift it right by 2.
This translation is illustrated below.

Translations, together, with linear transformations, make up the broader class of transformations called ``affine transformations."
These are transformations of the form $T: \mathbb{R}^2 \to \mathbb{R}^2$, $T(X) = AX + b$ where $A$ is an $n\times n$ matrix and $b \in \mathbb{R}^n$.
Affine transformations include all compositions of scalings, rotations, dilations, reflections, and translations.
For example, if $S$ represents a shear and $R$ a rotation, and if $\mathbf{b}$ is a vector in $\mathbb{R}^2$, then $T(\mathbf{x}) = RS\mathbf{x} + \mathbf{b}$ first shears $\mathbf{x}$, then rotates it, and finally translates it by $\mathbf{b}$.


\begin{figure}[H] % Generated with translated_horse() in plots.py.
\captionsetup[subfigure]{justification=centering}
\centering
\begin{subfigure}{.32\textwidth}
    \centering
    \includegraphics[width=\linewidth]{OriginalHorse.pdf}
\end{subfigure}%
\begin{subfigure}{.32\textwidth}
    \centering
    \includegraphics[width=\linewidth]{TranslationHorse.pdf}
\end{subfigure}
\end{figure}

Translations are usually \textbf{not} linear maps.
Therefore, they cannot be represented with matrix multiplication.

\begin{problem}
The moon orbits the earth and the earth orbits the sun.
Assuming circular orbits, we can compute the trajectories of the earth and moon using only linear and affine transformations.

Suppose an orientation where both the earth and moon travel counterclockwise, with the sun at the origin.
Let $p_e(t)$ and $p_m(t)$ be the positions of the earth and the moon at time $t$, and let $\omega_e$ and $\omega_m$ be each celestial body's angular momentum (how fast they are orbiting).
We compute the positions as follows:

\begin{enumerate}
\item Compute the position of the earth at time $t$, $p_e(t)$, by rotating the initial vector $p_e(0)$ counterclockwise about the origin by $t\omega_e$ radians.
\item Compute the position of the moon \emph{relative to the earth} at time $t$ by rotating the vector $p_m(0) - p_e(0)$ counterclockwise about the origin by $t\omega_m$ radians.
\item Translate the resulting vector from the previous step by the location of the earth at time $t$, $p_e(t)$.
\end{enumerate}

Write a function that accepts a final time $T$ and the angular momenta $\omega_e$ and $\omega_m$.
Assuming initial positions $p_e(0) = (10,0)$ and $p_m(0) = (11,0)$, plot $p_e(t)$ and $p_m(t)$ over the time interval $t \in [0, T]$.
\\
% (Hints: Time will \textbf{not} be one of the axes of the final plot. Create two $(2 \times N)$ arrays, one for the earth and one for the moon, with the same structure as the horse picture---$x$ coordinates on the first row and $y$ coordinates on the second.)

The moon rotates around the earth approximately 13 times every year.
With $T = 2\pi$, $\omega_e = 1$, and $\omega_m = 13$, your plot should resemble the following figure.
%
\begin{figure}[H]
    \includegraphics[width=.7\textwidth]{SolarSystem.pdf}
\end{figure}

\label{prob:solar-system-trajectories}
\end{problem}

\begin{comment}
\begin{problem}
Imagine a particle $p_1$ rotating around a second particle $p_2$ which is moving through $\mathbb{R}^2$ in a straight line.
Suppose $p_2$ begins at the origin and $p_1$ begins at $(1, 0)$.
We can compute the trajectory of $p_1$ using affine transformations.

\begin{enumerate}\label{prob:trajectory}
\item Write a function that returns the position of $p_1$ at a time $t$.
Your function should accept a time $t$, an angular velocity $\omega$, a direction vector $\mathbf{v}$, and a speed $s$.
Assume $p_1$ rotates with angular velocity $\omega$ and $p_2$ moves in the direction of $\mathbf{v}$ with speed $s$.
The location of $p_1$ at time $t$ can be computed as follows:
\begin{itemize}
\item Calculate the position of $p_2$ at time $t$ with the formula $(st/\|\mathbf{v}\|) \mathbf{v}$.
\item Calculate the position of $p_1$ as follows:
\begin{itemize}
\item Rotate $p_1$ by $t\omega$ radians.
\item Translate the resulting vector by the vector equal to the position of $p_2$ at time $t$.
\end{itemize}
\end{itemize}
\end{enumerate}
\item Plot the trajectory of $p_1$ on the time interval $(0, 10)$ assuming $\omega=\pi$, $v=(1, 1)$, and $s=3$.

Your graph should resemble the following figure.
\begin{figure}[H]
    \includegraphics[width=\textwidth]{trajectory.pdf}
\end{figure}
\end{problem}
\end{comment}

\newpage

\section*{Additional Material} % ==============================================



