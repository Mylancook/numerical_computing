\lab{QR 2: Applications}{Applications of the QR Decomposition}
\label{lab:qr-applications}
\objective{Because of its numerical stability and convenient structure, the QR decomposition is the basis of many important and practical algorithms.
In this lab, we introduce linear least squares problems, tools in Python for computing least squares solutions, and two fundamental eigenvalue algorithms.
\\ \indent As in the previous lab, we restrict ourselves to real matrices and therefore use the transpose in place of the Hermitian conjugate.}

\section*{Least Squares} % ====================================================

A linear system $A\x = \b$ is \emph{overdetermined} if it has more equations than unknowns.
In this situation, there is no true solution, and $\x$ can only be approximated.

The \emph{least squares solution} of $A\x = \b$, denoted as $\widehat{\x}$, is the ``closest'' vector to a solution, meaning it minimizes the quantity $\|A\widehat{\x} - \b\|_2$.
% \footnote{The choice of the 2-norm is significant.}
In other words, $\widehat{\x}$ is the vector such that $A\widehat{\x}$ is projection of $\b$ onto the range of $A$, and can be calculated by solving the \emph{normal equation}:%
\footnote{See Volume 1 Chapter 3 for a formal derivation of the normal equation.}
\[A\trp A\widehat{\x} = A\trp \b\]

If $A$ is full rank, which it usually is in applications, its QR decomposition provides an efficient way to solve the normal equation.
Let $A = \widehat{Q}\widehat{R}$ be the reduced QR decomposition of $A$, so $\widehat{Q}$ is $m \times n$ with orthonormal columns and $\widehat{R}$ is $n \times n$, invertible, and upper triangular.
Since $\widehat{Q}\trp \widehat{Q} = I$, and since $\widehat{R}\trp$ is invertible, the normal equation can be reduced as follows (we omit the hats on $\widehat{Q}$ and $\widehat{R}$ for clarity):
%
\begin{align}
\nonumber
A\trp A\widehat{\x} &= A\trp \b \\ \nonumber
(Q R)\trp Q R  \widehat{\x}
&= (Q R)\trp \b \\ \nonumber
 R\trp Q\trp Q R  \widehat{\x}
&=  R\trp Q\trp \b \\ \nonumber
 R\trp R \widehat{\x}
&=  R\trp Q\trp \b \\
 R \widehat{\x}
&= Q\trp \b \label{eq:normal-equation-via-qr}
\end{align}

Thus $\widehat{\x}$ is the least squares solution to $A\x=\b$ if and only if $\widehat{R}\widehat{\x} = \widehat{Q}\trp\b.$
Since $\widehat{R}$ is upper triangular, this equation can be solved quickly with back substitution.

\begin{problem} % Solve the normal equations with QR.
Write a function that accepts an $m \times n$ matrix $A$ of rank $n$ and a vector $\b$ of length $n$.
Use the QR decomposition and Equation \ref{eq:normal-equation-via-qr} to solve the normal equation corresponding to $A\x = \b$.

You may use either SciPy's QR routine or one of your own routines from the previous lab.
In addition, you may use \li{la.solve_triangular()}, SciPy's optimized routine for solving triangular systems.

\label{prob:lstsq-via-qr}
\end{problem}

\subsection*{Fitting a Line} % ------------------------------------------------

The least squares solution can be used to find the best-fit curve of a chosen type to a set of points.
Consider the problem of finding the line $y = ax + b$ that best fits a set of $m$ points $\{(x_k, y_k)\}_{k=1}^m$.
Ideally, we seek $a$ and $b$ such that $y_k = ax_k + b$ for all $k$.
The following linear system simultaneously represents all of these equations.
%
\begin{equation}
A\x =
\left[\begin{array}{cc}
x_1 & 1 \\
x_2 & 1 \\
x_3 & 1 \\
\vdots & \vdots \\
x_m & 1
\end{array}\right]
\left[\begin{array}{c} a \\ b \end{array}\right]
=
\left[\begin{array}{c} y_1 \\ y_2 \\ y_3 \\ \vdots \\ y_m \end{array}\right]
= \b
\label{eq:linear-least-squares}
\end{equation}
%
Note that $A$ has full column rank as long as not all of the $x_k$ values are the same.

Because this system has two unknowns, it is guaranteed to have a solution if it has two or fewer equations.
However, if there are more that two data points, the system is overdetermined if any set of three points are not collinear.
We therefore seek a least squares solution, which in this case means finding the slope $\widehat{a}$ and $y$-intercept $\widehat{b}$ such that the line $y = \widehat{a}x+\widehat{b}$ best fits the data.

Figure \ref{fig:line-fit-example} is a typical example of this idea where $\widehat{a} \approx \frac{1}{2}$ and $\widehat{b} \approx -3$.

\begin{figure}[H] % Linear regression example (without code).
    \includegraphics[width=.59\textwidth]{figures/line_fit_example.pdf}
    \caption{}
    \label{fig:line-fit-example}
\end{figure}

% TODO: Consider replacing this data set with something with less variance.
% This data set is also used in the Data Visualization lab.
\begin{problem} % Linear regression of height vs weight.
The file \texttt{MLB.npy} contains measurements from over 1,000 recent Major League Baseball players, compiled by UCLA.\footnote{See \url{http://wiki.stat.ucla.edu/socr/index.php/SOCR_Data_MLB_HeightsWeights}.}
Each row in the array represents a different player; the columns are the player's height (in inches), weight (in pounds), and age (in years), in that order.

Find the least squares line that relates the height of the players to their weight (i.e., let height be the $x$ values and weight be the $y$ values).
%
\begin{enumerate}
    \item Construct the matrix $A$ and the vector $\b$ described by Equation \ref{eq:linear-least-squares}.\\
    (Hint: the functions \li{np.vstack()}, \li{np.column_stack()}, and/or \li{np.ones()} may be helpful.)
    \item Use your function from Problem \ref{prob:lstsq-via-qr} to find the least squares solution.
    \item Plot the data points as a scatter plot.
    \item Plot the least squares line with the scatter plot.\\
    (Hint: Make a new domain of $x$ values with \li{np.linspace()}, and use this domain to calculate $y = \widehat{a}x + \widehat{b}$.)
\end{enumerate}
\end{problem}

\begin{info} % scipy.stats.linregress().
The least squares problem of fitting a line to a set of points is often called \emph{linear regression}, and the resulting line is called the \emph{linear regression line}.
SciPy's specialized tool for linear regression is \li{scipy.stats.linregress()}.
This function takes in an array of $x$-coordinates and a corresponding array of $y$-coordinates, and returns the slope and intercept of the regression line, along with a few other statistical measurements.

For example, the following code produces Figure \ref{fig:line-fit-example}.

\begin{lstlisting}
>>> import numpy as np
>>> from scipy.stats import linregress

# Generate some random data close to the line y = .5x - 3.
>>> x = np.linspace(0, 10, 20)
>>> y = .5*x - 3 + np.random.randn(20)

# Use linregress() to calculate m and b, as well as the correlation
# coefficient, p-value, and standard error. See the documentation for
# details on each of these extra return values.
>>> a, b, rvalue, pvalue, stderr = linregress(x, y)

>>> plt.plot(x, y, 'k*', label="Data Points")
>>> plt.plot(x, a*x + b, 'b-', lw=2, label="Least Squares Fit")
>>> plt.legend(loc="upper left")
>>> plt.show()
\end{lstlisting}
\end{info}

\subsection*{Fitting a Polynomial} % ------------------------------------------

Least squares can also be used to fit a set of data to the best-fit polynomial of a specified degree.
Let $\{(x_k, y_k)\}_{k=1}^m$ be the set of $m$ data points in question.
The general form for a polynomial of degree $n$ is as follows:
\[
p_n(x) = c_n x^n + c_{n-1} x^{n-1} + \cdots + c_2 x^2 + c_1 x + c_0
\]
Note that the polynomial is uniquely determined by its $n+1$ coefficients $\{c_k\}_{k=0}^n$.
Ideally, then, we seek the set of coefficients $\{c_k\}_{k=0}^n$ such that
\[
y_k = c_n x_k^n + c_{n-1} x_k^{n-1} + \cdots + c_2 x_k^2 + c_1 x_k + c_0
\]
for all values of $k$.
These $m$ linear equations yield the following linear system:
\begin{equation}
A\x =
\left[\begin{array}{cccccc}
x_1^n & x_1^{n-1} & \cdots & x_1^2 & x_1 & 1 \\
x_2^n & x_2^{n-1} & \cdots & x_2^2 & x_2 & 1 \\
x_3^n & x_3^{n-1} & \cdots & x_3^2 & x_3 & 1 \\
\vdots & \vdots & & \vdots & \vdots & \vdots \\
x_m^n & x_m^{n-1} & \cdots & x_m^2 & x_m & 1 \\
\end{array}\right]
\left[\begin{array}{c}
c_n \\ c_{n-1} \\ \vdots \\ c_2 \\ c_1 \\ c_0
\end{array}\right]
=
\left[\begin{array}{c} y_1 \\ y_2 \\ y_3 \\ \vdots \\ y_m \end{array}\right]
= \b
\label{eq:polynomial-least-squares}
\end{equation}
%
If $m > n+1$ this system is overdetermined, requiring a least squares solution.

\subsubsection*{Working with Polynomials in NumPy} % - - - - - - - - - - - - -

The $m \times (n+1)$ matrix $A$ of Equation \ref{eq:polynomial-least-squares} is called a \emph{Vandermonde matrix}.%
\footnote{Vandermonde matrices have many special properties and are useful for many applications, including polynomial interpolation and discrete Fourier analysis.}
% a matrix with entries $a_{ij} = x_i^{n-j+1}$.
NumPy's \li{np.vander()} is a convenient tool for quickly constructing a Vandermonde matrix, given the values $\{x_k\}_{k=1}^m$ and the number of desired columns.

\begin{lstlisting}
>>> print(np.vander([2, 3, 5], 2))
[[2 1]
 [3 1]
 [5 1]]

>>> print(np.vander([2, 3, 5, 4], 3))
[[ 4  2  1]
 [ 9  3  1]
 [25  5  1]
 [16  4  1]]
\end{lstlisting}

NumPy also has powerful tools for working efficiently with polynomials.
The class \li{np.poly1d} represents a 1-dimensional polynomial.
Instances of this class are callable like a function.%
\footnote{Class instances can be made callable by implementing the \li{__call__()} magic method.}
The constructor accepts the polynomial's coefficients, from largest degree to smallest.

Table \ref{table:numpy-poly1d} lists the attributes and methods of the \li{np.poly1d} class.
See \url{http://docs.scipy.org/doc/numpy/reference/routines.polynomials.html} for a list of NumPy's polynomial routines.

\begin{table}[H]
\begin{tabular}{r|l}
    Attribute & Description \\
    \hline
    coeffs & The $n+1$ coefficients, from greatest degree to least. \\
    order & The polynomial degree ($n$). \\
    roots & The $n-1$ roots. \\
    \\
    Method & Returns \\
    \hline
    deriv() & The coefficients of the polynomial after being differentiated. \\
    integ() & The coefficients of the polynomial after being integrated (with $c_0 = 0$).
\end{tabular}
\caption{Attributes and methods of the \li{np.poly1d} class.}
\label{table:numpy-poly1d}
\end{table}
%
\begin{lstlisting}
# Create a callable object for the polynomial f(x) = (x-1)(x-2) = x^2 - 3x + 2.
>>> f = np.poly1d([1, -3, 2])
>>> print(f)
   2
1 x - 3 x + 2

# Evaluate f(x) for several values of x in a single function call.
>>> f([1, 2, 3, 4])
array([0, 0, 2, 6])

# Evaluate f(x) at 1, 2, 3, and 4 without creating f(x) explicitly.
>>> np.polyval([1, -3, 2], [1, 2, 3, 4])
array([0, 0, 2, 6])
\end{lstlisting}

\begin{problem} % Polynomial fitting.
The file \texttt{polynomial.npy} contains an array of $20$ $(x,y)$ coordinate pairs.
Each row of the array is a different coordinate pair, with $x$ values in the first column and $y$ values in the second column.

Write a function that uses Equation \ref{eq:polynomial-least-squares} to calculate the polynomials of degree $3$, $5$, $7$, and $19$ that best fit the data.
Plot the original data points and each least squares polynomial together in individual subplots.\\
(Hint: set the window limits to be the same on each subplot to get an accurate picture of what's happening with the degree $19$ polynomial).

Instead of using Problem \ref{prob:lstsq-via-qr} to solve the normal equation, you may use \li{scipy.linalg.lstsq()}, demonstrated below.
% Compare your results to \li{np.polyfit()}.
% This function receives arrays of $x$ and $y$ values and an integer for the degree, and returns the coefficients of the best-fit polynomial of that degree.

\begin{lstlisting}
>>> from scipy import linalg as la

# Define A and b appropriately.

# Solve the normal equation using SciPy's least squares routine.
# The least squares solution is the first of four return values.
>>> x = la.lstsq(A, b)[0]
\end{lstlisting}

Compare your results to \li{np.polyfit()}.
This function receives an array of $x$ values, an array of $y$ values, and an integer for the polynomial degree, and returns the coefficients of the best-fit polynomial of that degree.

\label{prob:polynomial-least-squares}
\end{problem}

\begin{warn} % Overfitting
Problem \ref{prob:polynomial-least-squares} demonstrates that having more parameters in a least squares model is not always better.
The polynomial of degree 19 actually \emph{interpolates} the data set, meaning that $p_{19}(x_k) = y_k$ exactly for each $k$, because there are enough unknowns that the system is no longer overdetermined.

Choosing to have too many unknowns in a fitting problem is (fittingly) called \emph{overfitting}, and is an important issue to avoid in any statistical model.
\end{warn}

\subsection*{Fitting a Circle} % ----------------------------------------------

Suppose the set of $m$ points $\{(x_k, y_k)\}_{k=1}^m$ are arranged in a nearly circular pattern.
The general equation of a circle with radius $r$ and center $(c_1, c_2)$ is as follows:
\begin{equation}
(x-c_1)^2 + (y-c_2)^2 = r^2.
\label{eq:standard-circle}
\end{equation}

The circle is uniquely determined  by $r$, $c_1$, and $c_2$, so these are the parameters that should be solved for in a least squares formulation of the problem.
However, Equation \ref{eq:standard-circle} is not linear in any of these variables.
%
\begin{align}
\nonumber (x - c_1)^2 + (y - c_2)^2 &= r^2 \\
\nonumber x^2 - 2c_1 x + c_1^2 + y^2 - 2c_2y + c_2^2 &= r^2 \\
x^2 + y^2 & = 2c_1 x + 2c_2 y + \textcolor{red}{r^2 - c_1^2 - c_2^2}
\label{eq:circle-expanded}
\end{align}

The quadratic terms $x^2$ and $y^2$ are acceptable because the points $\{(x_k, y_k)\}_{k=1}^m$ are given.
To eliminate the nonlinear terms in the unknown parameters $r$, $c_1$, and $c_2$, define a new variable $c_3 = r^2 - c_1^2 - c_2^2$.
Then for each point $(x_k, y_k)$, Equation \ref{eq:circle-expanded} becomes the following:
\[2c_1x_k + 2c_2y_k + \textcolor{red}{c_3} = x_k^2 + y_k^2\]
These $m$ equations are linear in $c_1$, $c_2$, and $c_3$, and can be written as a linear system.

\begin{equation}
\left[\begin{array}{ccc}
2 x_1 & 2 y_1 & 1 \\
2 x_2 & 2 y_2 & 1 \\
\vdots & \vdots & \vdots \\
2 x_m & 2 y_m & 1
\end{array}\right]
\left[\begin{array}{c} c_1 \\ c_2 \\ c_3 \end{array}\right]
=
\left[\begin{array}{c}
x_1^2 + y_1^2 \\
x_2^2 + y_2^2 \\
\vdots \\
x_m^2 + y_m^2
\end{array}\right]
\label{eq:circle-least-squares}
\end{equation}

After solving for the least squares solution, $r$ can be recovered with the relation $r = \sqrt{c_1^2 + c_2^2 + c_3}$.
Finally, plotting a circle is best done with polar coordinates.
Using the same variables as before, the circle can be represented in polar equations with the following equations:
\begin{align*}
x = r\cos(\theta) + c_1 && y = r\sin(\theta) + c_2,
\end{align*}
where $\theta \in [0, 2\pi]$.

\begin{lstlisting}
# Load some data and construct the matrix A and the vector b.
>>> xk, yk = np.load("circle.npy").T
>>> A = np.column_stack((2*x, 2*y, np.ones_like(x)))
>>> b = xk**2 + yk**2

# Calculate the least squares solution and calculate the radius.
>>> c1, c2, c3 = la.lstsq(A, b)[0]
>>> r = np.sqrt(c1**2 + c2**2 + c3)

# Plot the circle using polar coordinates.
>>> theta = np.linspace(0, 2*np.pi, 200)
>>> x = r*np.cos(theta) + c1
>>> y = r*np.sin(theta) + c2
>>> plt.plot(x, y, '-', lw=2)
>>> plt.plot(xk, yk, 'k*')
>>> plt.axis("equal")
\end{lstlisting}

\begin{figure}[H]
    \includegraphics[width=.55\textwidth]{figures/circle_fit_example.pdf}
\end{figure}

\begin{problem}
The general equation for an ellipse is \[ax^2 + bx + cxy + dy + ey^2 = 1.\]
Write a function that calculates the parameters for the ellipse that best fits the data in the file \texttt{ellipse.npy}.
Plot the original data points and the ellipse together, using the following function to plot the ellipse.

\begin{lstlisting}
def plot_ellipse(a, b, c, d, e):
    """Plot an ellipse of the form ax^2 + bx + cxy + dy + ey^2 = 1."""
    theta = np.linspace(0, 2*np.pi, 200)
    cos_t, sin_t = np.cos(theta), np.sin(theta)
    A = a*(cos_t**2) + c*cos_t*sin_t + e*(sin_t**2)
    B = b*cos_t + d*sin_t
    r = (-B + np.sqrt(B**2 + 4*A))/(2*A)

    plt.plot(r*cos_t, r*sin_t, lw=2)
    plt.gca().set_aspect("equal", "datalim")
\end{lstlisting}
\end{problem}

\section*{Computing Eigenvalues} % ============================================

The eigenvalues of a matrix are the roots of its characteristic polynomial.
Thus, to find the eigenvalues of an $n \times n$ matrix, we must compute the roots of a degree-$n$ polynomial.
This is easy for small $n$.
For example, if $n=2$ the quadratic equation can be used to find the eigenvalues.
However, Abel's Impossibility Theorem says that no such formula exists for the roots of a polynomial of degree 5 or higher.

% TODO: get rid of this formal statement, as we provide no proof.
\begin{theorem}[Abel's Impossibility Theorem]
There is no general algebraic solution for solving a polynomial equation of degree $n\geq5$.
\label{thm:Abel}
\end{theorem}

Thus, it is impossible to write an algorithm that will exactly find the eigenvalues of an arbitrary matrix.
(If we could write such an algorithm, we could also use it to find the roots of polynomials, contradicting Abel's theorem.)
This is a significant result.
It means that we must find eigenvalues with \emph{iterative methods}, methods that generate sequences of approximate values converging to the true value.

\subsection*{The Power Method} % ----------------------------------------------

There are many iterative methods for finding eigenvalues.
The power method finds an eigenvector corresponding to the \emph{dominant} eigenvalue of a matrix, if such an eigenvalue exists.
The dominant eigenvalue of a matrix is the unique eigenvalue of greatest magnitude.

To use the power method on a matrix $A$, begin by choosing a vector $\x_0$ such that $\|\x_0\|=1$
Then recursively define
\[
x_{k+1}=\frac{Ax_k}{\norm{Ax_k}}.
\]
If
\begin{itemize}
\item $A$ has a dominant eigenvalue $\lambda$, and
\item the projection of $\x_0$ into the subspace spanned by the eigenvectors corresponding to $\lambda$ is nonzero,
\end{itemize}
then the vectors $\x_0, \x_1, \x_2, \ldots$ will converge to an eigenvector of $A$ corresponding to $\lambda$.
(See [TODO: ref textbook] for a proof when $A$ is semisimple, or [TODO: ref something else] for a proof in the general case.)

If all entries of $A$ are positive, then $A$ will always have a dominant eigenvalue (see [TODO: ref something!] for a proof).
There is no way to guarantee that the second condition is met, but if we choose $\x_0$ randomly, it will almost always satisfy this condition.

Once you know that $\x$ is an eigenvector of $A$, the corresponding eigenvalue is equal to the \emph{Rayleigh quotient}
\[
\lambda = \frac{\langle Ax, x \rangle}{\|\x\|^2}.
\]

\begin{problem} % Implement the power method.
Write a function that implements the power method to compute an eigenvector
Your function should
\begin{enumerate}
\item Accept a matrix and a tolerance \li{tol}.
\item Start with a random vector.
\item Use the 2-norm wherever a norm is needed (use \li{la.norm()}).
\item Repeat the power method until the vector changes by less than the tolerance
In mathematical notation, you are defining $x_0, x_1, \ldots x_k$, and your function should stop when $\|x_{k+1}-x_k\| < \text{tol}$.
\item Return the found eigenvector and the corresponding eigenvalue (use \li{np.inner()}).
\end{enumerate}
Test your function on positive matrices.
\end{problem}

\begin{comment}
An overview of the proof of the method is that you can write a matrix in Jordan Conical form $A=VJV^{-1}$ where $V$ is the matrix of the generalized eigenspaces.
But the first column is is the eigenvector corresponding to largest eigenvalue and $J$ is a upper trianglar matrix of eigenvalues and ones.
Note that $A^k=VJ^kV^{-1}$
The limit as $k \rightarrow \infty$ of $(\frac{1}{\lambda_1}J)^k$ is a matrix of all zeros except for a one in the upper right hand corner.
So $(\frac{A}{\norm{A}})^k \approx VJ^kV^{-1}$ So the largest eigenvalue dominates.
\end{comment}

\subsection*{The QR Algorithm} % ----------------------------------------------

The disadvantage of the power method is that it only finds the largest eigenvector and a corresponding eigenvalue.
To use the QR algorithm, let $A_0=A$
Then let $Q_kR_k$ be the QR decomposition of $A_k$, and recursively define
\[
A_{k+1}=R_kQ_k.
\]
Then $A_0, A_1, A_2, \ldots $ will converge to a matrix of the form
\begin{equation*}
\label{eq:Schur form}
S =
     \begin{pmatrix}
          S_1 &* & \cdots & * \\
           0     &S_2  &  \ddots & \vdots \\
           \vdots  & \ddots & \ddots & *  \\
           0 & \cdots & 0 & S_m
    \end{pmatrix}
\end{equation*}
where $S_i$ is a $1\times1$ or $2\times2$ matrix.\footnote{If $S$ is upper triangular (i.e., all $S_i$ are $1\times1$ matrices), then $S$ is the \emph{Schur form} of $A$.
If some $S_i$ are $2\times2$ matrices, then $S$ is the \emph{real Schur form} of $A$.}
The eigenvalues of $A$ are the eigenvalues of the $S_i$.

This algorithm works for three reasons
First,
\[
Q_k^{-1}A_kQ_k = Q_k^{-1}(Q_kR_k)Q_k = (Q_k^{-1}Q_k)(R_kQ_k) = A_{k+1},
\]
so $A_k$ is similar to $A_{k+1}$.
Because similar matrices have the same eigenvalues, $A_k$ has the same eigenvalues as $A$.
Second, each iteration of the algorithm transfers some of the ``mass'' from the lower to the upper triangle.
This is what makes $A_0, A_1, A_2, \ldots$ converge to a matrix $S$ which has the described form.
Finally, since $S$ is block upper triangular, its eigenvalues are just the eigenvalues of its diagonal blocks (the $S_i$).

A $2 \times 2$ block will occur in $S$ when $A$ is real but has complex eigenvalues.
In this case, the complex eigenvalues occur in conjugate pairs, each pair corresponding to a $2 \times 2$ block on the diagonal of $S$.

% TODO: Put the Hessenberg stuff from the last lab here?
\subsubsection*{Hessenberg Preconditioning} % - - - - - - - - - - - - - - - - -

Often, we ``precondition'' a matrix by putting it in upper Hessenberg form before passing it to the QR algorithm.
This is always possible because every matrix is similar to an upper Hessenberg matrix (see Lab \ref{}).
Hessenberg preconditioning is done for two reasons.

First, the QR algorithm converges much faster on upper Hessenberg matrices because they are already close to triangular matrices.

Second, an iteration of the QR algorithm can be computed in $\mathcal{O}(n^2)$ time on an upper Hessenberg matrix, as opposed to $\mathcal{O}(n^3)$ time on a regular matrix.
This is because so many entries of an upper Hessenberg matrix are 0.
If we apply the QR algorithm to an upper Hessenberg matrix $H$, then this speed-up happens in each iteration of the algorithm, since if $H = QR$ is the QR decomposition of $H$ then $RQ$ is also upper Hessenberg.


\begin{problem}
Write a function that implements the QR algorithm with Hessenberg preconditioning as described above.
Do this as follows.
\begin{enumerate}
\item Accept a matrix \li{A}, a number of iterations \li{niter}, and a tolerance \li{tol}.
\item Put \li{A} in Hessenberg form using \li{la.hessenberg()}.
\item Compute the matrix $S$ by performing the QR algorithm \li{niter} times.
Use the function \li{la.qr()} to compute the QR decomposition.
\item Iterate through the diagonal of $S$ from top to bottom to compute its eigenvalues.
For each diagonal entry,
\begin{enumerate}
\item If this is the last diagonal entry, then it is an eigenvalue.
\item If the entry below this one has absolute value less than \li{tol}, assume this is a $1\times 1$ block.
Then the current entry is an eigenvalue.
\item Otherwise, the current entry is at the top left corner of a $2 \times 2$ block.
Calculate the eigenvalues of this block.
Use the \li{sqrt} function from the scimath library to find the square root of a negative number.
You can import this library with the line \li{from numpy.lib import scimath}.
\end{enumerate}
\item Return the (approximate) eigenvalues of \li{A}.
\end{enumerate}
You can check your function on the matrix
\[
\begin{pmatrix}
 4 &  12 & 17 &  -2 \\
-5.5& -30.5 & -45.5 &  9.5\\
 3
&  20
& 30
&  -6
\\
1.5 &  1.5&   1.5&   1.5
       \end{pmatrix},
\]
which has eigenvalues $1+2i, 1-2i, 3$, and 0
You can also check your function on random matrices against \li{la.eig()}.
\label{prob:qr_solver}
\end{problem}


\begin{comment}
\begin{problem}
\label{prob:QR_eig_hessenberg}
Write a version of the QR algorithm that performs the QR algorithm by computing the Hessenberg form of a matrix, then computing various QR decompositions of the Hessenberg form of the matrix.
Use your solutions to \ref{prob:hessenberg} (where you computed the Hessenberg form of a matrix) and Problem \ref{prob:givens_hessenberg_modified} to do the necessary computations (where you computed the QR decomposition of a Hessenberg matrix and wrote code for multiplication by $Q$ that works in $\mathcal{O} \left( n^2 \right)$ time).
The solution to Problem \ref{prob:givens_hessenberg_modified} is especially important because it allows the compution of each QR decomposition and each $R Q = \left( Q^T R^T \right)$ in $\mathcal{O} \left( n^2 \right)$ time.
\end{problem}
\end{comment}

\begin{comment}
\begin{problem}
If $A$ is normal, its Schur form is diagonal.
For normal $A$, have your function additionally output the eigenvector corresponding to each eigenvalue.
Hint 1: Test your function on Hermitian and real symmetric matrices; they are both normal.
Hint 2: Your work in Problem \ref{problem:similarity proof} will help.
You have already made all the necessary calculations, you just need to store the information correctly.
\end{problem}
\end{comment}

\begin{comment}
\begin{problem}
Test your implementation with random matrices.
Try real-valued and symmetric matrices.
Compare your output to the output from the eigenvalue solver.
How many iterations are necessary?
How large can $A$ be?
\end{problem}
\end{comment}

The QR algorithm as described in this lab is not often used.
Instead, modern computer packages use the implicit QR algorithm, which is an improved version of the QR algorithm.

Lastly, iterative methods besides the power method and QR method are often used to find eigenvalues.
Arnoldi iteration is similar to the QR algorithm but exploits sparsity.
Other methods include the Jacobi method and the Rayleigh quotient method.

\begin{comment}
\newpage

\section*{Additional Material} % ==============================================

\subsection*{Weighted Least Squares} % ----------------------------------------

In these Least Squares problems, we have found best fit lines and ellipses relative to the 2-norm.
It is possible to generalize the idea of best fit curves relative to other norms.
See Figure \ref{Fig:ellipse} for an illustration of this.

\begin{figure}[h]
\label{ellipsefit}
\centering
\includegraphics[width=\textwidth]{figures/ellipsefit.pdf}
\caption{Fitting an ellipse using different norms.}
\label{Fig:ellipse}
\end{figure}

\subsection*{Improvements to the QR Algorithm} % ------------------------------

\end{comment}
