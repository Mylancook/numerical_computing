\lab{QR 1: Decomposition}{The QR Decomposition}
\label{lab:QRdecomp}
\def\lvl#1{\multicolumn{1}{|c}{#1}} % Left  Vertical Line in array cell.
\def\rvl#1{\multicolumn{1}{c|}{#1}} % Right Vertical Line in array cell.
\objective{The QR decomposition is a fundamentally important matrix factorization.
It is straightforward to implement, is numerically stable, and provides the basis of several important algorithms.
In this lab, we explore several ways to produce the QR decomposition and implement a few immediate applications.
}

The QR decomposition of a matrix $A$ is a factorization $A=QR$, where $Q$ is  has orthonormal columns and $R$ is upper triangular.
Every $m \times n$ matrix $A$ of rank $n \le m$ has a QR decomposition, with two main forms.
%
\begin{itemize}
    \item \textbf{Reduced QR}: $Q$ is $m \times n$, $R$ is $n \times n$, and the columns of $Q$ $\{\q_j\}_{j=1}^n$ form an orthonormal basis for the column space of $A$.
    \item \textbf{Full QR}: $Q$ is $m \times m$ and $R$ is $m \times n$.
    In this case, the columns of $Q$ $\{\q_j\}_{j=1}^m$ form an orthonormal basis for all of $\mathbb{F}^m$, and the last $m - n$ rows of $R$ only contain zeros.
    If $m = n$, this is the same as the reduced factorization.
\end{itemize}
We distinguish between these two forms by writing $\widehat{Q}$ and $\widehat{R}$ for the reduced decomposition and $Q$ and $R$ for the full decomposition.
%
\begin{align*} % Reduced and full QR decompositions.
\begin{array}{ccc}
\textcolor{red}{\widehat{Q}\ (m \times n)} & \textcolor{blue}{\widehat{R}\ (n \times n)} & \\
\left[\begin{array}{cccccccc}
\arrayrulecolor{red}
\cline{2-4}
& \lvl{}     &        & \rvl{}     &          &        &      & \\
& \lvl{}     &        & \rvl{}     &          &        &      & \\
& \lvl{}     &        & \rvl{}     &          &        &      & \\
& \lvl{\q_1} & \cdots & \rvl{\q_n} & \q_{n+1} & \cdots & \q_m & \\
& \lvl{}     &        & \rvl{}     &          &        &      & \\
& \lvl{}     &        & \rvl{}     &          &        &      & \\
& \lvl{}     &        & \rvl{}     &          &        &      & \\
\cline{2-4}
\end{array}\right]
&
\left[\begin{array}{ccccc}
\arrayrulecolor{blue}
\cline{2-4}
& \lvl{r_{11}} & \cdots & \rvl{r_{1n}} & \\
& \lvl{}       & \ddots & \rvl{\vdots} & \\
& \lvl{}       &        & \rvl{r_{nn}} & \\
\cline{2-4}
& 0            & \cdots & 0            & \\
& \vdots       &        & \vdots       & \\
& 0            & \cdots & 0            & \\
\end{array}\right]
& =\ A\ (m \times n)
\\
Q\ (m \times m) & R\ (m \times n) & \\
\end{array}
\end{align*}

We will assume throughout this lab that $A$ contains only real numbers.
However, the results and algorithms presented here can be extended to complex matrices by replacing ``transpose'' with ``hermitian conjugate'' and ``symmetric matrix'' with ``hermitian matrix.''

\section*{QR via Gram-Schmidt} % ==============================================

The \emph{classical Gram-Schmidt algorithm} takes a linearly independent set of vectors and constructs an orthonormal set of vectors with the same span.
Applying Gram-Schmidt to the columns of $A$, which are linearly independent since $A$ has rank $n$, results in the columns of $Q$.

Let $\{\x_j\}_{j=1}^n$ be the columns of $A$.
Define
%
\begin{align*}
\q_1 = \frac{\x_1}{\|\x_1\|},
&&
\q_{k} = \frac{\x_k - \p_{k-1}}{\|\x_k - \p_{k-1}\|},\quad k=2,\ \ldots,\ n,
\\ \\
\p_0 = \0,\ \text{and}
&&
\p_{k-1} = \sum_{j=1}^{k-1} \langle \q_j, \x_k\rangle \q_j,\quad k=2,\ \ldots,\ n.
\end{align*}

Each $\p_{k-1}$ is the projection of $\x_k$ onto the span of $\{\q_j\}_{j=1}^{k-1}$, so $\q_k' = \x_k - \p_{k-1}$ is the residual vector of the projection.
Thus $\q_k'$ is orthogonal to each of the $\{\q_j\}_{j=1}^{k-1}$.
Therefore, normalizing each $\q_k'$ produces an orthonormal set $\{\q_j\}_{j=1}^n$.

To construct the reduced QR decomposition, let $\widehat{Q}$ be the matrix with columns $\{\q_j\}_{j=1}^n$, and let $\widehat{R}$ be the upper triangular matrix with the following entries:
%
\begin{align*}
r_{kk} = \|\x_k-\p_{k-1}\|,
&&
r_{jk} = \langle \q_j, \x_k\rangle = \q_j\trp\x_k,\ j < k.
\end{align*}
%
This clever choice of entries for $\widehat{R}$ reverses the Gram-Schmidt process and ensures that $\widehat{Q}\widehat{R} = A$.

\begin{comment}
To construct the full QR decomposition, choose $m - n$ vectors $\{\x_j\}_{j=n+1}^m$ such that the entire set of original vectors $\{\x_j\}_{j=1}^m$ is linearly independent, then continue the Gram-Schmidt process to produce the additional columns of $Q$.
Appending $m - n$ rows of zeros to $\widehat{R}$ results in $R$.
\end{comment}

\subsection*{Modified Gram-Schmidt} % -----------------------------------------

When implemented with a computer, the Gram-Schmidt algorithm may produce a set of vectors $\{\q_j\}_{j=1}^n$ that are not even close to orthonormal due to rounding errors.
The \emph{modified Gram-Schmidt algorithm} is a slight variant of the classical algorithm which consistently produces a set of vectors that are ``very close'' to orthonormal.

Let $\q_1$ be the normalization of $\x_1$ as before.
Instead of making just $\x_2$ orthogonal to $\q_1$, make \emph{each} of the vectors $\{\x_j\}_{j=2}^n$ orthogonal to $\q_1$:
\[\x_k = \x_k - \langle \q_1,\x_{k} \rangle \q_1,\quad k=2,\ \ldots,\ n.\]
Next, define $\q_2 = \frac{\x_2}{\|\x_2\|}$.
Proceed by making each of $\{\x_j\}_{j=3}^n$ orthogonal to $\q_2$:
\[\x_k = \x_k - \langle \q_2,\x_{k} \rangle \q_2,\quad k=3,\ldots,n.\]
Since each of these new vectors is a linear combination of vectors orthogonal to $\q_1$, they are orthogonal to $\q_1$ as well.
Continuing this process results in the desired orthonormal set $\{\q_j\}_{j=1}^n$.
The entire modified Gram-Schmidt algorithm is described below in Algorithm \ref{Alg:modified-gram-schmidt}.

\begin{algorithm}[H]
\begin{algorithmic}[1]
\Procedure{Modified Gram-Schmidt}{$A$}
    \State $m, n \gets \shape{A}$
        \Comment{Store the dimensions of $A$.}
    \State $Q \gets \makecopy{A}$
        \Comment{Make a copy of $A$ with \li{np.copy()}.}
    \State $R \gets \zeros{n}{n}$
        \Comment{An $n\times n$ array of all zeros.}
    \For{$i=0\ldots n-1$}
        \State $R_{i,i} \gets \|Q_{:,i}\|$\label{step:mgs-normalize}
            % \Comment{(Hint: use \li{scipy.linalg.norm()}).}
        \State $Q_{:,i} \gets Q_{:,i}/R_{i,i}$\label{step:mgs-mult1}
            \Comment{Normalize the $i$th column of $Q$.}
        \For{$j=i+1\ldots n-1$}
            \State $R_{i,j} \gets Q_{:,j}\trp  Q_{:,i}$\label{step:mgs-mult2}
            \State $Q_{:,j} \gets Q_{:,j}-R_{i,j}Q_{:,i}$\label{step:mgs-mult3}
                \Comment{Orthogonalize the $j$th column of $Q$.}
        \EndFor
    \EndFor
    \State \pseudoli{return} $Q, R$
\EndProcedure
\end{algorithmic}
\caption{}
\label{Alg:modified-gram-schmidt}
\end{algorithm}

\begin{problem} % QR via Modified Gram Schmidt.
Write a function that accepts an $m \times n$ matrix $A$ of rank $n$.
Use Algorithm \ref{Alg:modified-gram-schmidt} to compute the reduced QR decomposition of $A$.

Consider the following tips for implementing the algorithm.
\begin{itemize}

\item In Python, the operation \li{a = a + b} can also be written as \li{a += b}.

\item Use \li{scipy.linalg.norm()} to compute the norm of the vector in step \ref{step:mgs-normalize}.

\item Note that steps \ref{step:mgs-mult1} and \ref{step:mgs-mult3} employ scalar multiplication or division, while step \ref{step:mgs-mult2} uses vector multiplication.

\end{itemize}

To test your function, generate test cases with NumPy's \li{np.random} module.
Verify that $R$ is upper triangular, $Q$ is orthonormal, and $QR = A$.
You may also want to compare your results to SciPy's QR factorization algorithm.

\begin{lstlisting}
>>> import numpy as np
>>> from scipy import linalg as la

# Generate a random matrix and get its reduced QR decomposition via SciPy.
>>> A = np.random.random((6,4))
>>> Q,R = la.qr(A, mode="economic") # Use mode="economic" for reduced QR.
>>> print A.shape, Q.shape, R.shape
(6,4) (6,4) (4,4)

# Verify that R is upper triangular, Q is orthonormal, and QR = A.
>>> np.allclose(np.triu(R), R)
True
>>> np.allclose(np.dot(Q.T, Q), np.identity(4))
True
>>> np.allclose(np.dot(Q, R), A)
True
\end{lstlisting}
\label{prob:qr-via-mgs}
\end{problem}

\newpage

\section*{Consequences of the QR Decomposition} % =============================

The special structures of $Q$ and $R$ immediately provide some simple applications.

\subsection*{Determinants} % --------------------------------------------------

Let $A$ be $n \times n$.
Then $Q$ and $R$ are both $n \times n$ as well.%
\footnote{An $n \times n$ orthonormal matrix is sometimes called \emph{unitary} in other texts.}
Since $Q$ is orthonormal and $R$ is upper-triangular,
\begin{align*} \det(Q) = \pm 1 && \det(R) = \prod_{i=1}^n r_{i,i} \end{align*}
Then since $\det(AB) = \det(A)\det(B)$, we have the following:
\[\abs{\det(A)} = \abs{\det(QR)} = \abs{\det(Q)\det(R)} = \abs{\prod_{k=1}^n r_{kk}}\]

\begin{problem} % Use the QR decomposition to calculate |det(A)|.
Write a function that accepts an invertible $n \times n$ matrix $A$.
Use the QR decomposition of $A$ to calculate $\abs{\det(A)}$.

You may use your QR decomposition algorithm from Problem \ref{prob:qr-via-mgs} or SciPy's QR routine.
Can you implement this function in a single line?
\end{problem}

\subsection*{Linear Systems} % ------------------------------------------------

The LU decomposition is usually the matrix factorization of choice to solve the linear system $A\x = \b$ because the triangular structures of $L$ and $U$ facilitate forward and backward substitution.
However, the QR decomposition avoids the potential numerical issues that come with Gaussian elimination.

Since $Q$ is orthonormal, $Q^{-1}= Q\trp$.
Therefore, solving $A\x = \b$ is equivalent to solving the system $R\x = Q\trp\b$.
Since $R$ is upper-triangular, $R\x = Q\trp\b$ can be solved quickly with back substitution.%
\footnote{See Problem 3 of the Linear Systems lab for a refresher on back substitution.}

\begin{problem} % Use the QR decomposition to solve Ax = b quickly.
Write a function that accepts an invertible $n \times n$ matrix $A$ and a vector $\b$ of length $n$.
Use the QR decomposition to solve $A\x = \b$ in the following steps:
\begin{enumerate}
    \item Compute $Q$ and $R$.
    \item Calculate $\y = Q\trp\b$.
    \item Use back substitution to solve $R\x = \y$ for $\x$.
\end{enumerate}
\end{problem}

\section*{QR via Householder} % ===============================================

The Gram-Schmidt algorithm orthonormalizes $A$ using a series of transformations that are stored in an upper triangular matrix.
Another way to compute the QR decomposition is to take the opposite approach: triangularize $A$ through a series of orthonormal transformations.
Orthonormal transformations are numerically stable, meaning that they are less susceptible to rounding errors.
In fact, this approach is usually faster and more accurate than Gram-Schmidt methods.

The idea is for the $k$th orthonormal transformation $Q_k$ to map the $k$th column of $A$ to the span of $\{\e_j\}_{j=1}^k$, where the $\e_j$ are the standard basis vectors in $\mathbb{R}^m$.
In addition, to preserve the work of the previous transformations, $Q_k$ should not modify any entries of $A$ that are above or to the left of the $k$th diagonal term of $A$.
For a $4 \times 3$ matrix $A$, the process can be visualized as follows.
%
\begin{align*}
Q_3 Q_2 Q_1
\left[\begin{array}{ccccc}
\cline{2-4}
& \lvl{*} & * & \rvl{*} & \\
& \lvl{*} & * & \rvl{*}& \\
& \lvl{*} & * & \rvl{*} & \\
& \lvl{*} & * & \rvl{*} & \\ \cline{2-4}
\end{array}\right]
= Q_3 Q_2
\left[\begin{array}{ccc}
     *  & * &      * \\
\cline{2-3}
\rvl{0} & * & \rvl{*} \\
\rvl{0} & * & \rvl{*} \\
\rvl{0} & * & \rvl{*} \\
\cline{2-3}
\end{array}\right]
= Q_3
\left[\begin{array}{ccc}
* & * & * \\
0 & * & * \\
\cline{3-3}
0 & \rvl{0} & \rvl{*} \\
0 & \rvl{0} & \rvl{*} \\
\cline{3-3}
\end{array}\right]
=
\left[\begin{array}{ccc}
* & * & * \\
0 & * & * \\
0 & 0 & * \\
0 & 0 & 0 \\
\end{array}\right]
\end{align*}

Thus $Q_3 Q_2 Q_1 A = R$, so that $A = Q_1\trp Q_2\trp Q_3\trp R$ since each $Q_k$ is orthonormal.
Furthermore, the product of square orthonormal matrices is orthonormal, so setting $Q = Q_1\trp Q_2\trp Q_3\trp$ yields the full QR decomposition.

How to correctly construct each $Q_k$ isn't immediately obvious.
The ingenious solution lies in one of the basic types of linear transformations: reflections.

\subsection*{Householder Transformations} % -----------------------------------

The \emph{orthogonal complement} of a nonzero vector $\v \in \mathbb{R}^n$ is the set of all vectors $\x \in \mathbb{R}^n$ that are orthogonal to $\v$, denoted $\v^\perp = \{ \x \in \mathbb{R}^n \mid \langle \x, \v \rangle = 0 \}$.
A \emph{Householder transformation} is a linear transformation that reflects a vector $\x$ across the orthogonal complement $\v^\perp$ for some specified $\v$.

The matrix representation of the Householder transformation corresponding to $\v$ is given by $H_{\v} = I - 2\frac{\v\v\trp}{\v\trp\v}$.
Since $H_{\v}\trp H_{\v} = I$, Householder transformations are orthonormal (can you prove it?).

\begin{figure}[H]
\centering
\begin{tikzpicture}
    \draw[-,dashed, gray](-2,-1.333)--(3, 2);               % hyperplane
    \draw[->, gray, >=stealth,ultra thick](0,0)--(.8,-1.2); % v
    \draw[->, >=stealth, thick](0,0)--(2.815, .777);        % H x
    \draw[->, >=stealth, thick](0,0)--(1.8,2.3);            % x
    \node[draw=none](v)at(.65,-.6){$\v$};
    \node[draw=none](x)at(.75,1.5){$\x$};
    \node[draw=none](Hx)at(3, .5){$H_{\v}\x$};
    \node[draw=none](H)at(-1.5,-.6){$\v^\perp$};
    % \node[draw=none](bullet)at(2.31, 1.53){\textbullet};
    % \node[draw=none](vandx)at(4.0,1.5){$\x -
    % \left \langle \dfrac{\v}{\|\v\|}, \x \right \rangle \dfrac{\v}{\|\v\|}$};
\end{tikzpicture}
\caption{The vector $\v$ defines the orthogonal complement $\v^\perp$.
Applying the Householder transformation $H_{\mathbf{v}}$ to $\x$ reflects $\x$ across $\v^\perp$.}
\label{fig:Householder_reflector}
\end{figure}

\subsection*{Householder Triangularization} % ---------------------------------

The \emph{Householder algorithm} uses Householder transformations for the orthonormal transformations in the QR decomposition process described on the previous page.
The goal in choosing $Q_k$ is to send $\x_k$, the $k$th column of $A$, to the span of $\{\e_j\}_{j=1}^k$.
In other words, if $Q_k\x_k = \y_k$, the last $m - k$ entries of $\y_k$ should be $0$.
%
\begin{align*}
Q_k\x_k = Q_k
\left[\begin{array}{c}
z_1 \\ \vdots \\ z_k \\ z_{k+1} \\ \vdots \\ z_m
\end{array}\right]
=
\left[\begin{array}{c}
y_1 \\ \vdots \\ y_k \\ 0 \\ \vdots \\ 0
\end{array}\right]
= \y_k
\end{align*}

To begin, decompose $\x_k$ into $\x_k = \x_k' + \x_k''$, where $\x_k'$ and $\x_k''$ are of the form
\[
\x_k' = \left[z_1\quad \cdots\quad z_{k-1}\quad 0\quad \cdots\quad 0\right]\trp
\qquad\text{and}\qquad
\x_k'' = \left[0\quad \cdots\quad 0\quad z_k\quad \cdots\quad z_m\right]\trp.
\]
Because $\x_k'$ represents elements of $A$ that lie above the diagonal, only $\x_k''$ needs to be altered by the reflection.

The two vectors $\x_k''\ \pm\ \|\x_k''\|\e_k$ both yield Householder transformations that send $\x_k''$ to the span of $\e_k$ (see Figure \ref{fig:householder-two-possible-reflectors}).
Between the two, the one that reflects $\x_k''$ further is more numerically stable.
This reflection corresponds to \[\v_k = \x_k'' + \sign(z_k)\|\x_k''\| \e_k,\] where $z_k$ is the first nonzero component of $\x_k''$ (the $k$th component of $\x_k$).

\begin{figure}[H] % Two possible reflections into the span of e_1.
\begin{tikzpicture}

\draw[-, dashed, gray](-3,-1)--(3,1);
\draw[-, dashed, gray](-.8,2.4)--(.6,-1.8);
\draw[-, gray, thick](-4,0)--(4,0);
\draw[->, thick, >=stealth'](0,0)--(2,1.6);
\draw[<->, thick, >=stealth'](-2.6,0)--(2.6,0);
\draw[->, gray,  ultra thick, >=stealth](0,0)--(1.5,.5);
\draw[->, gray, ultra thick, >=stealth](0,0)--(-.5,1.5);

\node[draw=none, node distance=3.5cm]
    (dummy)at(2.5,.2){};
\node[draw=none, node distance=.5cm](Hvx)
    [below of=dummy]{$H_{\v_1}\x$};
\node[draw=none, node distance=2cm](x)
    [above left of=Hvx]{$\x$};
\node[draw=none](v1)[left of=x]{$\v_1$};
\node[draw=none, node distance=.55cm](v2)[below of=x]{$\v_2$};
\node[draw=none, node distance=4cm](hvx2)[left of=dummy]{$H_{\v_2}\x$};

\end{tikzpicture}
\caption{There are two possible reflections that map $\x$ into the span of $\e_1$, defined by the vectors $\v_1$ and $\v_2$.
In this illustration, $H_{\v_2}$ is the more stable transformation since it reflects $\x$ further than $H_{\v_1}$.}
\label{fig:householder-two-possible-reflectors}
\end{figure}

After choosing $\v_k$, set $\u_k = \frac{\v_k}{\|\v_k\|}$.
Then $H_{\v_k} = I - 2\frac{\v_k\v_k\trp}{\|\v_k\|^2} = I - 2\u_k\u_k\trp$, and hence $Q_k$ is given by the following block matrix.
%
\begin{equation*}
Q_k =
\left[\begin{array}{cc}
I_{k-1} & \0 \\
\0      & H_{\v_k} \\
\end{array}\right] =
\left[\begin{array}{cc}
I_{k-1} & \0 \\
\0      & I_{m-k+1} - 2 \u_k\u_k\trp \\
\end{array}\right]
\end{equation*}

Here $I_{p}$ denotes the $p \times p$ identity matrix, and thus each $Q_k$ is $m \times m$.

\newpage

It is apparent from its form that $Q_k$ does not affect the first $k-1$ rows and columns of any matrix that it acts on.
Then by starting with $R = A$ and $Q = I$, at each step of the algorithm we need only multiply the entries in the lower right $(m-k+1) \times (m-k+1)$ submatrices of $R$ and $Q$ by $I-2\u_k\u_k\trp$.
This completes the Householder algorithm, detailed below.

\begin{algorithm}[H] % QR Decomposition via Householder Reflections.
\begin{algorithmic}[1]
\Procedure{Householder}{$A$}
    \State $m, n \gets \shape{A}$
    \State $R \gets \makecopy{A}$
    \State $Q \gets I_{m}$
        \Comment{The $m\times m$ identity matrix.}
    \For{$k=0\ldots n-1$}
        \State $\u \gets \makecopy{R_{k:,k}}$
        \State $u_0 \gets u_0 + \sign(u_0)\|\u\|$
            \Comment{$u_0$ is the first entry of $\u$.}\label{step:HH-sign}
        \State $\u \gets \u/\|\u\|$
            \Comment{Normalize $\u$.}
        \State $R_{k:,k:} \gets R_{k:,k:} - 2\u\left(\u\trp R_{k:,k:}\right)$
            \Comment{Apply the reflection to $R$.}\label{step:HH-outer1}
        \State $Q_{k:,:} \gets Q_{k:,:} - 2\u\left(\u\trp Q_{k:,:}\right)$
            \Comment{Apply the reflection to $Q$.}\label{step:HH-outer2}
    \EndFor
    \State \pseudoli{return} $Q\trp, R$
\EndProcedure
\end{algorithmic}
\caption{}
\label{Alg:QR-via-Householder}
\end{algorithm}

\begin{problem} % QR Decomposition via Householder Triangularization
Write a function that accepts as input a $m \times n$ matrix $A$ of rank $n$.
Use Algorithm \ref{Alg:QR-via-Householder} to compute the full QR decomposition of $A$.
Consider the following implementation details.
\begin{itemize}

\item NumPy's \li{np.sign()} is an easy way to implement the $\sign()$ operation in step \ref{step:HH-sign}.
However, \li{np.sign(0)} returns $0$, which will cause a problem in the rare case that $u_0 = 0$ (which is possible if the top left entry of $A$ is $0$ to begin with).
The following code defines a function that returns the sign of a single number, counting $0$ as positive.

\begin{lstlisting}
sign = lambda x: 1 if x >= 0 else -1
\end{lstlisting}

\item In steps \ref{step:HH-outer1} and \ref{step:HH-outer2}, the multiplication of $\u$ and $(\u\trp X)$ is an \emph{outer product} ($\x\y\trp$ instead of the usual $\x\trp\y$).
Use \li{np.outer()} instead of \li{np.dot()} to handle this correctly.

\end{itemize}

As with Problem \ref{prob:qr-via-mgs}, use NumPy and SciPy to generate test cases and validate your function.

\begin{lstlisting}
>>> A = np.random.random((5, 3))
>>> Q,R = la.qr(A)                  # Get the full QR decomposition.
>>> print A.shape, Q.shape, R.shape
(5,3) (5,5) (5,3)
>>> np.allclose(Q.dot(R), A)
True
\end{lstlisting}
\label{prob:qr-via-hessenberg}
\end{problem}

\section*{Upper Hessenberg Form} % ============================================

An \emph{upper Hessenberg matrix} is a square matrix that is nearly upper triangular, with zeros below the first subdiagonal.
Every $n \times n$ matrix $A$ can be written $A = Q\trp HQ$ where $Q$ is orthonormal and $H$ is an upper Hessenberg matrix, called the \emph{Hessenberg form} of $A$.
Putting a matrix in upper Hessenberg form is an important first step to computing its eigenvalues numerically.

This algorithm also uses Householder transformations.
To find orthogonal $Q$ and upper Hessenberg $H$ such that $A = Q\trp HQ$, it suffices to find such matrices that satisfy $Q A Q\trp = H$.
Thus, the strategy is to multiply $A$ on the left and right by a series of orthonormal matrices until it is in Hessenberg form.
Using the same $Q_k$ as in the $k$th step of the Householder algorithm introduces $n - k$ zeros in the $k$th column of $A$, but multiplying $Q_k A$ on the right by $Q_k\trp$ destroys all of those zeros.

Instead, choose a $Q_1$ that fixes $\e_1$ and reflects the first column of $A$ into the span of $\e_1$ and $\e_2$.
The product $Q_1A$ then leaves the first row of $A$ alone, and the product $(Q_1A)Q_1\trp$ leaves the first column of $(Q_1A)$ alone.
\[
\begin{array}{ccccc}
\left[\begin{array}{ccccc}
* & * & * & * & * \\
* & * & * & * & * \\
* & * & * & * & * \\
* & * & * & * & * \\
* & * & * & * & *
\end{array}\right]
&\underrightarrow{Q_1}&
\left[\begin{array}{ccccc}
* & * & * & * & * \\
* & * & * & * & * \\
0 & * & * & * & * \\
0 & * & * & * & * \\
0 & * & * & * & *
\end{array}\right]
&\underrightarrow{Q_1\trp }&
\left[\begin{array}{ccccc}
* & * & * & * & * \\
* & * & * & * & * \\
0 & * & * & * & * \\
0 & * & * & * & * \\
0 & * & * & * & *
\end{array}\right]
\\
A & & Q_1A & & (Q_1 A) Q_1\trp
\end{array}
\]
Continuing the process results in the upper Hessenberg form of $A$.
%
\begin{equation*}
Q_3 Q_2 Q_1 A Q_1\trp Q_2 \trp Q_3\trp =
\left[\begin{array}{ccccc}
* & * & * & * & * \\
* & * & * & * & * \\
0 & * & * & * & * \\
0 & 0 & * & * & * \\
0 & 0 & 0 & * & *
\end{array}\right]
\end{equation*}

This implies that $A = Q_1\trp Q_2\trp Q_3\trp H Q_3 Q_2 Q_1$, so setting $Q = Q_3 Q_2 Q_1$ results in the desired factorization $A = Q\trp H Q$.

\subsection*{Constructing the Reflections} % ----------------------------------

Constructing the $Q_k$ uses the same approach as in the Householder algorithm, but shifted down one element.
Let $\x_k = \y_k' + \y_k''$ where $\y_k'$ and $\y_k''$ are of the form
\[
\y_k' = \left[z_1\quad \cdots\quad z_k\quad 0\quad \cdots\quad 0\right]\trp
\qquad\text{and}\qquad
\y_k'' = \left[0\quad\cdots\quad 0\quad z_{k+1}\quad\cdots\quad z_m\right]\trp.
\]
Because $\y_k'$ represents elements of $A$ that lie above the first subdiagonal, only $\y_k''$ needs to be altered.
This suggests using the following reflection.
%
\begin{align*}
\v_k = \y_k'' + \sign(z_k)\|\y_k''\| \e_k
&&
\u_k = \frac{\v_k}{\|\v_k\|}
\end{align*}
\[
Q_k =
\left[\begin{array}{cc}
I_k & \0 \\
\0      & H_{\v_k} \\
\end{array}\right] =
\left[\begin{array}{cc}
I_k & \0 \\
\0      & I_{m-k} - 2 \u_k\u_k\trp \\
\end{array}\right]
\]

The complete algorithm is given below.
Note how similar it is to Algorithm \ref{Alg:QR-via-Householder}.
% Although the Hessenberg form exists for any square matrix, this algorithm only works for full-rank square matrices.

\begin{algorithm}[H]
\begin{algorithmic}[1]
\Procedure{Hessenberg}{$A$}
    \State $m, n \gets \shape{A}$
    \State $H \gets \makecopy{A}$
    \State $Q \gets I_{m}$
    \For{$k=0 \ldots n-3$}
        \State $\u \gets \makecopy{H_{k+1:,k}}$
        \State $u_0 \gets u_0 + \sign(u_0)\|\u\|$
        \State $\u \gets \u/\|\u\|$
        \State $H_{k+1:,k:} \gets H_{k+1:,k:} - 2\u(\u\trp H_{k+1:,k:})$
            \Comment{Apply $Q_k$ to $H$.}
        \State $H_{:,k+1:} \gets H_{:,k+1:} - 2(H_{:,k+1:} \u) \u\trp$
            \Comment{Apply $Q_k\trp$ to $H$.}
        \State $Q_{k+1:,:} \gets Q_{k+1:,:} - 2\u(\u\trp Q_{k+1:,:})$
            \Comment{Apply $Q_k$ to $Q$.}
    \EndFor
    \State \pseudoli{return} $Q, H$
\EndProcedure
\end{algorithmic}
\caption{}
\label{Alg:Upper-Hessenberg-Form}
\end{algorithm}

\begin{problem} % Reduce a matrix to upper Hessenberg form.
Write a function that accepts a nonsingular $n \times n$ matrix $A$.
Use Algorithm \ref{Alg:Upper-Hessenberg-Form} to compute its upper Hessenberg form, returning orthogonal $Q$ and upper Hessenberg $H$ satisfying $A = Q\trp HQ$.

Test your function and compare your results to \li{scipy.linalg.hessenberg()}.

\begin{lstlisting}
# Generate a random matrix and get its upper Hessenberg form via SciPy.
>>> A = np.random.random((8,8))
>>> H = la.hessenberg(A)

# Verify that H has all zeros below the first subdiagonal.
>>> np.allclose(np.triu(H, -1), H)
True
\end{lstlisting}

\label{prob:hessenberg-via-householder}
\end{problem}

\begin{info}
When $A$ is symmetric, its upper Hessenberg form is a tridiagonal matrix.
This is because the $Q_k$'s zero out everything below the first subdiagonal of $A$ and the $Q_k\trp$'s zero out everything to the right of the first superdiagonal.
Tridiagonal matrices make computations fast, so the computing the Hessenberg form of a symmetric matrix is very useful.
\end{info}

\newpage

\section*{Additional Material} % ==============================================

\subsection*{Givens Rotations} % ----------------------------------------------

In the previous section we found the QR decomposition of a matrix using Householder transformations, applying a series of these transformations to a matrix until it was in upper triangular form.
We can use the same strategy to compute the QR decomposition with rotations instead of reflections.

Let us begin with Givens rotations in $\mathbb{R}^2$.
An arbitrary vector $\x = (a, b)\trp$ can be rotated into the span of $\e_1$ via an orthogonal transformation.
In fact, the matrix $T_{\theta} = \left[\begin{array}{cc}\cos \theta & - \sin \theta \\ \sin \theta & \cos \theta \end{array}\right]$ rotates a vector counterclockwise by $\theta$.
Thus, if $\theta$ is the clockwise-angle between $\x$ and $\e_1$, the vector $T_{-\theta}\x$ will be in the span of $\e_1$.
We can find $\sin \theta$ and $\cos \theta$ with the formulas $\sin = \frac{\text{opp}}{\text{hyp}}$ and $\cos = \frac{\text{adj}}{\text{hyp}}$, so $\sin \theta = \frac{b}{\sqrt{a^2+b^2}}$ and $\cos \theta =  \frac{a}{\sqrt{a^2+b^2}}$ (see Figure\ref{fig:angle}).
Then
\[
T_{-\theta}\x
=
\left[\begin{array}{cc}
\cos\theta  & \sin\theta \\
-\sin\theta & \cos\theta \\
\end{array}\right]
\left[\begin{array}{c} a \\ b \end{array}\right]
=
\left[\begin{array}{cc}
 \frac{a}{\sqrt{a^2+b^2}} & \frac{b}{\sqrt{a^2+b^2}} \\
-\frac{b}{\sqrt{a^2+b^2}} & \frac{a}{\sqrt{a^2+b^2}} \\
\end{array}\right]
\left[\begin{array}{c} a \\ b \end{array}\right]
=
\left[\begin{array}{c} \sqrt{a^2+b^2} \\ 0 \end{array}\right].
\]

\begin{figure}[H]
\begin{center}
\begin{tikzpicture}
\draw[-, thick](-.5,0)--(3,0);
\draw[-, thick](0,-.5)--(0,1.5);
\draw[->, thick, >=stealth'](0,0)--(2.5,1);
\draw[-,thick](2.5,-.2)--(2.5,.2);
\draw[-, thick](-.2,1)--(.2,1);
\node[draw=none](point_b)at(2.5,-.4){$a$};
\node[draw=none](point_a)at(-.4,1){$b$};
\draw[-, thick] (.5,.2)arc [start angle=60,
    end angle=-20, radius=4.5pt];
\node[draw=none](theta)at(.8, .15){$\theta$};
\end{tikzpicture}
\caption{Rotating clockwise by $\theta$ will send the vector $(a,b)\trp$ to the span of $\e_1$.}
\label{fig:angle}
\end{center}
\end{figure}

The matrix $T_{\theta}$ above is an example of a $2 \times 2$ Givens rotation matrix.
In general, the Givens matrix $G(i,j,\theta)$ represents the orthonormal transformation that rotates the 2-dimensional span of $\e_i$ and $\e_j$ by $\theta$ radians.
The matrix for this transformation is
\begin{equation*}
G(i,j,\theta) =
\left[\begin{array}{ccccc}
I & 0 & 0 & 0 & 0 \\
0 & c & 0 & -s & 0 \\
0 & 0 & I & 0 & 0 \\
0 & s & 0 & c & 0 \\
0 & 0 & 0 & 0 & I
\end{array}\right].
\end{equation*}
This matrix is in block form with $I$ representing the identity matrix, $c=\cos \theta$, and $s=\sin \theta$.
The $c$'s appear on the $i^{th}$ and $j^{th}$ diagonal entries.

As before, we can choose $\theta$ so that $G(i,j,\theta)$ rotates a given vector so that its $\e_j$-component is 0.
Such a transformation will only affect the $i^{th}$ and $j^{th}$ entries of any vector it acts on (and thus the $i^{th}$ and $j^{th}$ rows of any matrix it acts on).

This flexibility makes Givens rotations ideal for some problems.
For example, Givens rotations can be used to solve linear systems defined by sparse matrices by modifying only small parts of the array.
Also, Givens rotations can be used to solve systems of equations in parallel.

The advantages of Givens rotations are that they orthonormal and hence numerically stable (like Householder reflections), and they affect only a small part of the array (like Gaussian elimination).
The disadvantage is that they require a greater number of floating point operations than Householder reflections.
% Accuracy and Stability of Numerical Algorithms, Nicholas J. Higham
In practice, the Givens algorithm is slower than the Householder algorithm, even when it is modified to decrease the number of floating point operations.
% Fast Plane Rotations With Dynamic Scaling, Anda and Park, SIAM, 1994
However, since Givens rotations can be parallelized, they can be much faster than the Householder algorithm when multiple processors are used.
% Givens and Householder Reductions for Linear Least Squares on a Cluster of Workstations, Omer Egecioglu and Ashok Srinivasan

\subsection*{Givens Triangularization} % --------------------------------------

We can apply Givens rotations to a matrix until it is in upper triangular form, producing a factorization $A = QR$ where $Q$ is a composition of Givens rotations and $R$ is upper triangular.
This is exactly the QR decomposition of $A$.

The idea is to iterate through the subdiagonal entries of $A$ in the order depicted by Figure \ref{fig:givens}.
We zero out the $ij^{th}$ entry with a rotation in the plane spanned by $\e_{i-1}$ and $\e_i$.
This rotation is just multiplication by the Givens matrix $G(i-1,i,\theta)$, which can be computed as in the example at the start of the previous section.
We just set $a=a_{i-1,j}$ and $b=a_{i,j}$, so $c = \cos \theta = a/\sqrt{a^2+b^2}$ and $s = \sin \theta = -b/\sqrt{a^2+b^2}$.

\begin{figure}[H]
\begin{center}
\begin{tikzpicture}[xscale=.7, yscale=.7]
\draw[->, gray, thick](0,0)--(0,5.5);
\draw[->, gray, thick](1,0)--(1,4.5);
\draw[->, gray, thick](2,0)--(2,3.5);
\draw[->, gray, thick](3,0)--(3,2.5);
\draw[->, gray, thick](4,0)--(4,1.5);
\node[draw=none] at(0,-1){\large 1};
\node[draw=none] at(1,-1){\large 2};
\node[draw=none] at(2,-1){\large 3};
\node[draw=none] at(3,-1){\large 4};
\node[draw=none] at(4,-1){\large 5};

\draw[-, ultra thick] (0,6)--(4,2);

\draw[-, thick](-1,-.5)--(-1,6.5);
\draw[-, thick](-1, -.5)--(-.5,-.5);
\draw[-, thick](-1,6.5)--(-.5,6.5);

\draw[-, thick](5,-.5)--(5,6.5);
\draw[-, thick](5,-.5)--(4.5,-.5);
\draw[-, thick](5,6.5)--(4.5,6.5);

%\draw[-, thick](-.5,0)--(3,0);
%\draw[-, thick](0,-.5)--(0,1.5);
%\draw[->, thick, >=stealth'](0,0)--(2.5,1);
%\draw[-,thick](2.5,-.2)--(2.5,.2);
%\draw[-, thick](-.2,1)--(.2,1);
%\node[draw=none](point_b)at(2.5,-.4){$a$};
%\node[draw=none](point_a)at(-.4,1){$b$};
%\draw[-, thick] (.5,.2)arc [start angle=60,
%   end angle=-20, radius=4.5pt];
%\node[draw=none](theta)at(.8, .15){$\theta$};
\end{tikzpicture}
\caption{The order in which to zero out subdiagonal entries in the Givens triangularization algorithm.
The heavy black line is the main diagonal of the matrix.
Entries should be zeroed out from bottom to top in each column, beginning with the leftmost column.}
\label{fig:givens}
\end{center}
\end{figure}

For example, on a $2 \times 3$ matrix we may perform the following operations:
\[
\left[\begin{array}{cc}
* & * \\
* & * \\
* & * \\
\end{array}\right]
\underrightarrow{G(2,3,\theta_1)}
\left[\begin{array}{cccc}
&      *  &      *  & \\
\cline{2-3}
& \lvl{*} & \rvl{*} & \\
& \lvl{0} & \rvl{*} & \\
\cline{2-3}
\end{array}\right]
\underrightarrow{G(1,2,\theta_2)}
\left[\begin{array}{cccc}
\cline{2-3}
& \lvl{*} & \rvl{*} & \\
& \lvl{0} & \rvl{*} & \\
\cline{2-3}
&      0  &      *  & \\
\end{array}\right]
\underrightarrow{G(2,3,\theta_3)}
\left[\begin{array}{ccc}
     * &       *  & \\
\cline{2-2}
\rvl{0} & \rvl{*} & \\
\rvl{0} & \rvl{0} & \\
\cline{2-2}
\end{array}\right]
\]
At each stage, the boxed entries are those modified by the previous transformation.
The final transformation $G(2,3,\theta_3)$ operates on the bottom two rows, but since the first two entries are zero, they are unaffected.
Assuming that at the $ij^{th}$ stage of the algorithm, $a_{ij}$ is nonzero, Algorithm \ref{Alg:givens} computes the Givens triangularization of a matrix..


\begin{algorithm}
\begin{algorithmic}[1]
\caption{Givens triangularization. Return an orthogonal matrix $Q$ and an upper triangular matrix $R$ satisfying $A = QR$.}
\label{Alg:givens}
\Procedure{Givens Triangularization}{$A$}
\State $m, n \gets \shape{A}$
\State $R \gets \makecopy{A}$
\State $Q \gets I_{m}$
\For{$j=0\ldots n-1$}
    \For{$i=m-1\ldots j+1$}
      \State $a, b \gets R_{i-1,j}, R_{i,j}$
      \State $G \gets [[a, b],[-b,a]]/\sqrt{a^2+b^2}$
      \State $R_{i-1:i+1,j:} \gets GR_{i-1:i+1, j:}$
      \State $Q_{i-1:i+1,:} \gets GQ_{i-1:i+1,:}$
    \EndFor
\EndFor
\State \pseudoli{return} $Q\trp , R$
\EndProcedure
\end{algorithmic}
\end{algorithm}

Notice that in Algorithm \ref{Alg:givens}, we \emph{do not} actually create the matrices $G(i,j,\theta)$ and multiply them by the original matrix.
Instead we modify only those entries of the matrix that are affected by the transformation. As an additional way to save memory, it is possible to modify this algorithm so that $Q$ and $R$ are stored in the original matrix $A$.
\begin{comment}
An interesting side-note is that each Givens rotation can be represented as a single floating point number, so, when operating in place, $Q$ can be stored entirely in the lower triangular portion of the array on which we are operating by storing each rotation in the entry that it zeroes out.
A similar approach would to store Householder reflectors in the columns they zero out.
In either case, we can represent the QR decomposition of an array using only the memory that was originally used to store the array itself.
This is similar to the approach  for computing the LU decomposition entirely in place.
These representations of $Q$ and $R$ can be used in various ways to perform matrix multiplication by $Q$, $Q\trp$ and $R$ as needed.
\end{comment}


\begin{comment}
\begin{itemize}[$\bullet$]

\item Make $R$ a copy of $A$ and $Q$ an identity array of the appropriate size.

\item Make an empty $2 \times 2$ array $G$ that will be used to apply the Givens rotations.

\item For each column:

  \begin{itemize}[$\bullet$]

  \item For each row below the main diagonal (starting at the bottom of the column):

    \begin{itemize}[$\bullet$]

    \item If the leading entry of this row is not zero (i.e. if its absolute value is within a given tolerance):

      \begin{itemize}[$\bullet$]

      \item Compute $c$ and $s$ using the entry in the current row and column and the entry immediately above it.

      \item Use $c$ and $s$ to construct the matrix $G$.

      \item Get a slice of $R$ of the current row and the row above it that includes the columns from the current column onward.
      Multiply it in place by $G$ to zero out the leading nonzero entry of the current row.

      \item Get a slice of $Q$ of the current row and the row above it and apply $G$ to it as well. (Strictly speaking, you do not need to operate over these entire rows, but the slicing needed to avoid the extra computation is a little more involved, so we will not include that here.)

      \end{itemize}

    \end{itemize}

  \end{itemize}

\item Return $Q\trp$ and $R$.

\end{itemize}
\end{comment}


\begin{problem}[Optional]
Write a function that computes the Givens triangularization of a matrix, using Algorithm \ref{Alg:givens}.
Assume that at the $ij^{th}$ stage of the algorithm, $a_{ij}$ will be nonzero.
\label{prob:Givens}
\end{problem}

\begin{problem}[Optional]
\label{prob:givens_hessenberg}
Modify your solution to Problem \ref{prob:Givens} to compute the Givens triangularization of an upper Hessenberg matrix, making the following changes:
\begin{enumerate}
    \item Iterate through the first subdiagonal from left to right. (These are the only entries that need to be zeroed out.)
    \item Line 11 of Algorithm \ref{Alg:givens} updates $Q$ with the current Givens rotation $G$. Decrease the number of entries of $Q$ that are modified in this line. Do this by replacing $Q_{i-1:i+1, :}$ with $Q_{i-1:i+1, :k_i}$ where $k_i$ (dependent on $i$) is chosen such that $Q_{i-1:i+1, k_i:}=0$. % k_i = i + 1.
\end{enumerate}
Hint: Here is how to generate a random upper Hessenberg matrix on which to test your function.
The idea is to generate a random matrix and then zero out all entries below the first subdiagonal.

\begin{lstlisting}
import numpy as np
import scipy.linalg as la
from math import sqrt
A = np.random.rand(500, 500)

# We do not need to modify the first row of A
# la.triu(A[1:]) zeros out all entries below the diagonal of A[1:]
A[1:] = la.triu(A[1:])

# A is now upper Hessenberg
\end{lstlisting}

\begin{comment}
What is the computational order of  complexity for this problem?
Approximately for what $m$ is your implementation as fast as the general QR decomposition built in to \li{scipy.linalg} for computing the QR decomposition of an upper Hessenberg matrix?
\end{comment}
\end{problem}

\begin{comment}
\begin{problem}
\label{prob:givens_hessenberg_modified}
You may have noticed that matrix multiplication by $Q$ is generally a $\mathcal{O} \left( n^3 \right)$ algorithm, while the application of these individual Givens rotations is a $\mathcal{O} \left( n^2 \right)$ algorithm.
Write a modified version of your solution to Problem \ref{prob:givens_hessenberg} called \li{givens2_mod} which returns
an $(n-1) \times 2 \times 2$ array containing the computed values for $G$ at each step in the algorithm in the order in which they are applied to the upper Hessenberg array $H$.

Write two more functions \li{apply_Q} and \li{apply_QT} which, using the matrix of Givens rotations, perform left multiplication
by $Q$ and $Q^{-1}$, respectively, on some other input array $B$.
Left multiplication by $Q^{-1}$ can be done by applying each of the Givens rotations to $B$ the same way you did to $H$ to compute its QR factorization.
Left multiplication by $Q$ can be done by applying the transpose of the Givens rotations to their corresponding portions of $B$, but in the reverse order.
Notice that you will have to apply each Givens rotation across the full width of the rows it operates on since you do not know anything about the content of $B$.

For around what size of matrices is direct multiplication by $Q$ slower than this method of multiplying by $Q$?
For timing purposes, make a random upper Hessenberg matrix, compute its QR decomposition using the function you just wrote and your solution to Problem \ref{prob:givens_hessenberg}, then time how long it takes to left multiply a random square array by $Q$ using the function you just wrote and the \li{dot} method of NumPy arrays.

Note: the functions you just wrote can be used to perform right multiplication as well since $B Q = \left(Q\trp B\trp \right)\trp$ and $B Q\trp = \left( Q B\trp \right)\trp$.
\end{problem}
\end{comment}

\begin{comment} % Move this to Additional Material?
\subsection*{Stability of the Householder QR algorithm} % ---------------------

We will now examine the stability of the Householder QR algorithm.
We will use SciPy's built in QR factorization which uses Householder reflections internally.

Try the following.

\begin{lstlisting}
>>> Q, X = la.qr(np.random.rand(500,500)) # create a random orthonormal matrix:
>>> R = np.triu(np.random.rand(500,500)) # create a random upper triangular matrix
>>> A = np.dot(Q,R) # Q and R are the exact QR decomposition of A
>>> Q1, R1 = la.qr(A) # compute QR decomposition of A
\end{lstlisting}

Observe:

\begin{lstlisting}
>>> la.norm(Q1-Q)/la.norm(Q) # check error in Q
0.282842955725
>>> la.norm(R1-R)/la.norm(R) # check error in R
0.0428922016647
\end{lstlisting}

This is terrible!
This algorithm works in $16$ decimal points of precision, but $Q_1$ and $R_1$ are only accurate to $0$ and $1$ decimal points, respectively.
We've lost $16$ decimal points of precision!

Don't lose hope.
Check how close the product $Q_1 R_1$ is to $A$.

\begin{lstlisting}
>>> A1 = Q1.dot(R1)
>>> np.absolute(A1 - A).max()
3.9968028886505635e-15
\end{lstlisting}

We've now recovered $15$ digits of accuracy.
Considering the error relative to the norm of $A$ (using the 2-norm for matrices), we see that this relative error is even smaller.

\begin{lstlisting}
>>> la.norm(A1 - A, ord=2) / la.norm(A, ord=2)
8.8655568331889288e-16
\end{lstlisting}

The errors in $Q_1$ and $R_1$ were somehow ``correlated," so that they canceled out in the product.
The errors in $Q_1$ and $R_1$ are called \emph{forward errors}.
The error in $A_1$ is the \emph{backward error}.

In fact, the large errors in \li{Q1} and \li{R1} were not because the algorithm was bad, it was because $A$ was poorly conditioned.
The condition number for randomly generated upper triangular matrices is generally very high, and this was the case here.
This has, in turn, made the condition number of $A$ extremely large.

Try the following to compute the condition number of $A$.
In this case the condition number of $A$ and $R$ are computed to be different, though, in theory, they should be exactly the same.

\begin{lstlisting}
>>> from numpy.linalg import cond
>>> cond(A)
4.1426075832870472e+18
>>> cond(R)
3.1767577244363792e+19
\end{lstlisting}

Householder QR factorization is more numerically stable than Gram-Schmidt or even Modified Gram-Schmidt (MGS).
However, MGS is still useful for some types of iterative methods because it finds the orthonormal basis one vector at a time instead of all at once (for an example see Lab \ref{lab:EigSolve}).
\end{comment}


%Sources: http://www.cs.unc.edu/~krishnas/eigen/node5.html
% http://en.wikipedia.org/wiki/Givens_rotation
%http://en.wikipedia.org/wiki/QR_decomposition
%   Note the Operation count: Householder is 2/3 n^3, MGS is 2 n^3
%http://en.wikipedia.org/wiki/QR_algorithm
%Applied Numerical methods using MATLAB by Yang has some code written for this
%http://www.math.kent.edu/~reichel/courses/intr.num.comp.2/lecture21/evmeth.pdf
%   These are eigenvalue algorithms explained carefully
%http://en.wikipedia.org/wiki/Householder_transformation
%Numerical Linear Algebra, by Lloyd N. Trefethen and David Bau III, Chapters 10 and 16

